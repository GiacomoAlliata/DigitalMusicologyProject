{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestone 2 - Data gathering and preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Narrowing down the research question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Research Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this research project we will try to analyse a corpus of popular songs to try to identify  chords differences between verses and choruses. We will try to  answer the following research questions :\n",
    "<ol>\n",
    "<li>Does the chord distribution of the choruses differ from the one in the verses?</li>\n",
    "<li>Is there a different chord sequence distribution in the choruses compared to the verses?</li>\n",
    "<li>How are these distributions evolving over time? </li>\n",
    "</ol>\n",
    "\n",
    "In the first question we want to know if a difference exists in terms of chords statistic. Are we more likely to find a specific chord in the choruses? Are less different chords used in the chorus compared to the other parts of a song? <br> \n",
    "For the second one we want to have a more melodic insight. Can we find specific patterns? Is the Markov model derived from the chorus different from its counterparts? <br>\n",
    "In the third one the focus is on the time dimension. We want to know if the results of the two previous answers change over time. Were chorus closer to verses in 1968 than in 1985? How does each chord evolve in relation to the others? Do we find a sequence that appeared while other disappeared? \n",
    "\n",
    "These are all the underlying questions we want to answer under our main research questions.  \n",
    "\n",
    "These questions relate to our original idea to fully characterize a chorus over the years, especially in comparison with verses. We now want to focus specifically on the chords to differentiate these different parts, while keeping the temporal dimension as a potential factor to observe changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset presentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To give us the means to answer our questions, we have selected a dataset containing approximately 900 Pop-Rock songs in the top Billboard charts from the 60s to the 90s. They are simple text files with the following informations : \n",
    "* release date\n",
    "* song title \n",
    "* artist name\n",
    "* labels for the different parts of the song (such as chorus or verse)\n",
    "* timestamp of each musical phrase beginning\n",
    "* chords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the chorus/verse annotations to classify the chords in each group. This will allow us to divide the chords in two groups and compute statistics and distributions for each of them. As discussed in the research questions, we will start with a basic characterization, simply comparing which chords appear in which section. We will then move deeper and compare the distributions of chords as well as the Markov models. These will be computed at least with bigrams, perhaps with higher-n n-grams depending on the number of chords in each section (as it makes little sense to use n-grams with n close to the number of chords in a given section).\n",
    "\n",
    "The metadata, especially the release date, will be used to study the evolution over time of the previously discussed statistics. Depending on the distribution of songs over the years, time analysis will be discussed either over years or over decades."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Possible outcomes and confidence measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The different outcomes we can reasonably expect are: \n",
    "<ul>\n",
    "    <li> <strong>Null results:</strong> There are no significant differences between choruses and verses and no evolution over time. This could be explained by a bias in the corpus toward a specific Pop-Rock genre using the same chords all the time or maybe there is indeed no difference in the chords used in a chorus and the ones used in verses, which would constitute an answer for our questions. </li>\n",
    "<li> <strong>Narrow chorus chord distribution:</strong> Since the chorus has to be immediately recognized as one, maybe the composers make more extensive use of a sub group of the chords to ensure it. The same reasoning could be applied to the chord sequences: perhaps some specific ones will be more dominant in the chorus.</li>\n",
    "<li> <strong>Temporal evolution:</strong> It will be interesting to see if the differences between verse and chorus change over the years. This evolution, if present, could be linear or oscillating. A linear narrowing would imply that choruses are becoming more and more similar or verse more and more diverse. An oscillating pattern would be interesting as musical phenomenon could appear and disappear.</li>\n",
    "</ul>\n",
    "\n",
    "Statistical tests will be used throughtout our analysis to check if our findings are statistically relevant. Error bars will also be included in all our graphics to avoid wrong conclusions. Of course the relative small size of our corpus will influence our results but only further analysis can reveal if significant results can still be found."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Gathering the data\n",
    "The dataset has been created by [1] and corresponds to a random sample of 890 Billboard chart slots presented at ISMIR 2011 and MIREX 2012. Due to the nature of the sampling algorithm, there are some duplicates and this results in only 740 distinct songs. According to the authors, training algorithms that assume independent, identically distributed data should retain the duplicates.<br> This dataset is publicly available at https://ddmal.music.mcgill.ca/research/The_McGill_Billboard_Project_(Chord_Analysis_Dataset)/ and can be downloaded in various formats. Different features are given by the authors. In this project we will use metadata and chords annotations. \n",
    "The first dataset used is the index to the dataset (csv format), containing the following fields:\n",
    "<ul>\n",
    "<li><b>id</b>, the index for the sample entry;</li>\n",
    "<li><b>chart_date</b>, the date of the chart for the entry;</li>\n",
    "<li><b>target_rank</b>, the desired rank on that chart;</li>\n",
    "<li><b>actual_rank</b>, the rank of the song actually annotated, which may be up to 2 ranks higher or lower than the target rank [1, 2];</li>\n",
    "<li><b>title</b>, the title of the song annotated;</li>\n",
    "<li><b>artist</b>, the name of the artist performing the song annotated;</li>\n",
    "<li><b>peak_rank</b>, the highest rank the song annotated ever achieved on the Billboard Hot 100; and</li>\n",
    "<li><b>weeks_on_chart</b>, the number of weeks the song annotated spent on the Billboard Hot 100 chart in total.</li>\n",
    "</ul>\n",
    "\n",
    "The main dataset comprehends chords, structure, instrumentation, and timing, given in a txt format. The annotation for each song begins with a header containing the title of the song, the name of the artist, the metre and the tonic pitch class of the opening key. In the main body, each line consists of a single phrase and begins with its timestamp, followed by the chords. This requires us to design a specific parser, as will be discussed in the next section.\n",
    "\n",
    "[1]: John Ashley Burgoyne, Jonathan Wild, and Ichiro Fujinaga, ‘An Expert Ground Truth Set for Audio Chord Recognition and Music Analysis’, in Proceedings of the 12th International Society for Music Information Retrieval Conference, ed. Anssi Klapuri and Colby Leider (Miami, FL, 2011), pp. 633–38"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this question is to load the data and have a look at it. A specific parser is designed to do this automatically, in order to extract and store in a Pandas dataframe all the relevant informations and musical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_df = pd.read_csv(\"data/billboard-2.0-index.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('There are %d entries in the index table.' %len(metadata_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('There are %d entries with a given title.' %metadata_df.title.isna().value_counts()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('There are %d entries with a given artist.' %metadata_df.artist.isna().value_counts()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('There are %d entries with a given chart date.' %metadata_df.chart_date.isna().value_counts()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "months = {'01':'January', \n",
    "          '02':'February',\n",
    "          '03':'March',\n",
    "          '04':'April',\n",
    "          '05':'May',\n",
    "          '06':'June',\n",
    "          '07':'July',\n",
    "          '08':'August',\n",
    "          '09':'September',\n",
    "          '10':'October',\n",
    "          '11':'November',\n",
    "          '12':'December'}\n",
    "def format_date(date):\n",
    "    year = date[:4]\n",
    "    month = date[5:7]\n",
    "    day = date[-2:]\n",
    "    if day == '01':\n",
    "        suffix = 'st'\n",
    "    elif day == '02':\n",
    "        suffix = 'nd'\n",
    "    elif day == '03':\n",
    "        suffix = 'rd'\n",
    "    else:\n",
    "        suffix = 'th'\n",
    "        \n",
    "    if day[0] == '0':\n",
    "        day = day[1]\n",
    "    \n",
    "    date_string = months[month] + ' ' + day + suffix + ', ' + year\n",
    "    return(date_string)\n",
    "\n",
    "#Test\n",
    "format_date('1958-08-04')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The songs range from %s to %s.' %(format_date(metadata_df.chart_date.min()), format_date(metadata_df.chart_date.max())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's analyse the song distribution over the year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_df['year'] = metadata_df['chart_date'].map(lambda date: int(date[:4]))\n",
    "n_bins = metadata_df.year.max() - metadata_df.year.min() + 1\n",
    "metadata_df.year.hist(bins = n_bins)\n",
    "plt.title('Song distribution over the years', fontsize = 20)\n",
    "plt.xlabel('Year', fontsize = 18)\n",
    "plt.ylabel('Number of songs', fontsize = 18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_number_songs = metadata_df.groupby(['year']).count().id.min()\n",
    "min_number_songs_year = metadata_df.groupby(['year']).count().id.idxmin()\n",
    "max_number_songs = metadata_df.groupby(['year']).count().id.max()\n",
    "max_number_songs_year = metadata_df.groupby(['year']).count().id.idxmax()\n",
    "mean_number_songs = metadata_df.groupby(['year']).count().id.mean()\n",
    "\n",
    "print('Year with the minimum number of songs: %d (with %d songs)' %(min_number_songs_year, min_number_songs))\n",
    "print('Year with the maximum number of songs: %d (with %d songs)' %(max_number_songs_year, max_number_songs))\n",
    "print('The average number of songs per year is %0.2f.' %mean_number_songs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now have a look at the artists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_artist = len(metadata_df.artist.unique()) - 1 #The -1 is to take into account the missing values in the dataframe\n",
    "print('There are %d different artists in the dataframe.' %number_artist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_number_songs_per_artist = metadata_df.groupby(['artist']).count().id.min()\n",
    "artist_min_songs = metadata_df.groupby(['artist']).count().id.idxmin()\n",
    "max_number_songs_per_artist = metadata_df.groupby(['artist']).count().id.max()\n",
    "artist_max_songs = metadata_df.groupby(['artist']).count().id.idxmax()\n",
    "#Since the distribution is highly skewed, we use the median instead of the mean here\n",
    "median_songs_per_artist = metadata_df.groupby('artist').count().id.median() \n",
    "artist_with_5songs = metadata_df.groupby('artist').count().id.map(lambda x: x >= 5).value_counts()[1]\n",
    "\n",
    "print('Artist with the lowest number of songs: %s (with %d songs)' %(artist_min_songs, min_number_songs_per_artist))\n",
    "print('Artist with the highest number of songs: %s (with %d songs)' %(artist_max_songs, max_number_songs_per_artist))\n",
    "print('The median number of songs per artist is %0.2f.' %median_songs_per_artist)\n",
    "print('Only %d artists have at least 5 songs in the corpus.' %artist_with_5songs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see with the distribution of the number of songs per artist, there are clearly some artist (such as Elvis Presley) with more than 10 songs in the corpus but the majority of them only has one or two songs. This results in a highly skewed distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_df.groupby('artist').count().id.hist(bins = max_number_songs_per_artist - min_number_songs_per_artist + 1)\n",
    "plt.title('Distribution of the number of songs per artist', fontsize = 20)\n",
    "plt.ylabel('Number of artists', fontsize = 18)\n",
    "plt.xlabel('Number of songs', fontsize = 18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note that the names of the artist are not easily readable (due to the high amount of different artists in the corpus).\n",
    "#This graph is only meant to show the diversity in the number of songs per artist, not precise information for each artist\n",
    "plt.figure(figsize=(18,10))\n",
    "metadata_df.groupby('artist').count().id.plot(kind = 'bar')\n",
    "plt.title('Number of songs for each artist', fontsize = 20)\n",
    "plt.xlabel('Artist', fontsize = 18)\n",
    "plt.ylabel('Number of songs', fontsize = 18)\n",
    "plt.xticks(fontsize = 6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parser "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SONG_ID, LINE_NUMBER, MEASURE_NUMBER, CHORD_NUMBER, SEQUENCE_NUMBER, \\\n",
    "CHORD, INSTRUMENT, TYPE, TIME, STRUCTURE, DURATION, REPETITION, ELID = \\\n",
    "\"song_id\",\"line_id\", \"measure_id\", \"chord_id\", \"sequence_id\",\\\n",
    "\"chord\", \"instrument\", \"section_type\", \"time\", \"section_structure\", \"duration\", \"repetition\", \"elided\"\n",
    "\n",
    "#This is dependant of \"metre\" in the txt files.\n",
    "METRE = \"metre\"\n",
    "\n",
    "#Create a new dictionary from two other\n",
    "def immutable_merge(dic1, dic2):\n",
    "    result = dic1.copy()\n",
    "    result.update(dic2)\n",
    "    return result\n",
    "\n",
    "#Create a row of the futur df as a dictionary\n",
    "def create_row(persistent_attributes, line_attributes,\n",
    "               measure_number = None, chord_number = None, chord = None, duration = None):\n",
    "    result = immutable_merge(persistent_attributes, line_attributes)\n",
    "    \n",
    "    if not (measure_number is None and measure_number is None and chord_number is None and duration is None):\n",
    "        result[MEASURE_NUMBER] = measure_number\n",
    "        result[CHORD_NUMBER] = chord_number\n",
    "        result[CHORD] = chord\n",
    "        result[DURATION] = duration\n",
    "    \n",
    "    return result\n",
    "\n",
    "#Generate the attributes of a given line and update the sequence counter\n",
    "def process_line_metadata(header, line_counter, old_line_attributes, sequence_counter, suffix = \"\"):\n",
    "    \n",
    "    result = {}\n",
    "    \n",
    "    #Suffix (main instrument, elid, repetition)\n",
    "    old_instrument = str(old_line_attributes.get(INSTRUMENT))\n",
    "    \n",
    "    for suffix in suffix.split(\", \"):\n",
    "        \n",
    "        suffix = suffix.strip()\n",
    "        \n",
    "        #Repetition\n",
    "        if re.match(\"^x\\d+$\",suffix):\n",
    "            result[REPETITION] = int(suffix[1])\n",
    "        \n",
    "        #Elid\n",
    "        elif suffix == \"->\":\n",
    "            result[ELID] = True\n",
    "\n",
    "        #Instrument\n",
    "        else:\n",
    "            ##New instrument\n",
    "            if len(suffix) > 0 and suffix != \"\\n\":\n",
    "                result[INSTRUMENT] = suffix.strip(\"\\n\").strip(\",\").strip()\n",
    "\n",
    "            ##Main instrument continued (experimental)\n",
    "            elif not old_instrument.endswith(\")\") and old_instrument.lower() not in [\"nan\",\"none\"] \\\n",
    "            and len(old_instrument)>0:\n",
    "                result[INSTRUMENT] = old_instrument.strip(\"(\")\n",
    "\n",
    "        \n",
    "    #Line number\n",
    "    result[LINE_NUMBER] = line_counter\n",
    "\n",
    "    \n",
    "    #Header    \n",
    "    header_items = header.split()\n",
    "        \n",
    "    result[TIME] = header_items[0]\n",
    "    \n",
    "    #Case where a section is continued\n",
    "    if len(header_items) == 1:\n",
    "        result[TYPE] = old_line_attributes.get(TYPE)\n",
    "        result[STRUCTURE] = old_line_attributes.get(STRUCTURE)\n",
    "        result[SEQUENCE_NUMBER] = old_line_attributes.get(SEQUENCE_NUMBER)\n",
    "    \n",
    "    #Case where a section has no structure (silence, end, fadeout)\n",
    "    elif len(header_items) == 2:\n",
    "        \n",
    "        #Z is a structure, not a type.\n",
    "        if header_items[1].strip().strip(\",\") == \"Z\":\n",
    "            result[STRUCTURE] = header_items[1].strip().strip(\",\")\n",
    "        else:\n",
    "            result[TYPE] = header_items[1].strip().strip(\",\")\n",
    "            \n",
    "        result[SEQUENCE_NUMBER] = sequence_counter\n",
    "        sequence_counter += 1\n",
    "    \n",
    "    #Case where a section begins.\n",
    "    elif len(header_items) == 3:\n",
    "        result[STRUCTURE] = header_items[1].strip().strip(\",\")\n",
    "        result[TYPE] = header_items[2].strip().strip(\",\")\n",
    "        result[SEQUENCE_NUMBER] = sequence_counter\n",
    "        sequence_counter += 1\n",
    "    \n",
    "    return sequence_counter, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_song_to_dict(song_id, path):\n",
    "    \n",
    "    rows = []\n",
    "    persistent_attributes = {}\n",
    "    \n",
    "    persistent_attributes[SONG_ID] = song_id\n",
    "    \n",
    "    with open(path,\"r\") as file:\n",
    "        line = file.readline()\n",
    "        \n",
    "        line_counter = 0\n",
    "        measure_counter = 0\n",
    "        chord_counter = 0\n",
    "        sequence_counter = 0\n",
    "        line_attributes = {}\n",
    "        old_chord = None\n",
    " \n",
    "\n",
    "        while line:\n",
    "        \n",
    "            if line != \"\\n\":\n",
    "\n",
    "                #Attribute lines\n",
    "                if line.startswith(\"#\"):\n",
    "                    attribute, value = line.strip(\"#\").split(\":\",1)\n",
    "                    persistent_attributes[attribute.strip(\" \")] = value.strip(\" \").strip(\"\\n\")\n",
    "\n",
    "                else:\n",
    "                    line_items = line.split(\"|\")\n",
    "\n",
    "                    #Special lines\n",
    "                    if len(line_items) <= 1:\n",
    "                        sequence_counter, line_attributes = \\\n",
    "                        process_line_metadata(line, line_counter, line_attributes, sequence_counter)\n",
    "                        row = create_row(persistent_attributes, line_attributes)\n",
    "                        rows.append(row)\n",
    "\n",
    "                    #Standard lines    \n",
    "                    else:                    \n",
    "                        header = line_items[0]\n",
    "                        suffix = line_items[-1]\n",
    "                        measures = line_items[1:-1]\n",
    "\n",
    "                        sequence_counter, line_attributes = \\\n",
    "                        process_line_metadata(header, line_counter, line_attributes, sequence_counter, suffix)  \n",
    "\n",
    "                        for measure in measures:\n",
    "                            \n",
    "                            chords = measure.split()\n",
    "                            \n",
    "                            #Special metric (experimental)\n",
    "                            old_metre = persistent_attributes.get(METRE)\n",
    "                            if re.match(\"^\\(\\d/\\d\\)$\", chords[0]):\n",
    "                                persistent_attributes[METRE] = str(chords[0][1]) + \"/\" + str(chords[0][3])\n",
    "                                chords = chords[1:]\n",
    "                            \n",
    "                            if len(chords) == 1:\n",
    "                                duration = \"measure\"\n",
    "                            else:\n",
    "                                duration = \"beat\"\n",
    "                            \n",
    "                            for chord in chords:\n",
    "                                \n",
    "                                if chord == \".\":\n",
    "                                    chord = old_chord\n",
    "                                \n",
    "                                row = create_row(persistent_attributes, line_attributes,\n",
    "                                                 measure_counter, chord_counter, chord, duration)\n",
    "                                rows.append(row)\n",
    "                                old_chord = chord\n",
    "                                chord_counter += 1\n",
    "\n",
    "                            measure_counter += 1\n",
    "                            persistent_attributes[METRE] = old_metre\n",
    "            \n",
    "            #Finally\n",
    "            line_counter += 1\n",
    "            line = file.readline()\n",
    "    \n",
    "    \n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.DataFrame(parse_song_to_dict(0,\"data/McGill-Billboard/0004/salami_chords.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_whole_collection_df():\n",
    "    \n",
    "    path = \"data/McGill-Billboard/\"\n",
    "    file_name = \"/salami_chords.txt\"\n",
    "    UPPER_BOUND = 1300\n",
    "    \n",
    "    whole_collection = []\n",
    "    \n",
    "    i = 0\n",
    "    while i <= UPPER_BOUND:\n",
    "        full_path = path + \"0\"*(4-len(str(i)))+ str(i) + file_name\n",
    "        \n",
    "        if os.path.exists(full_path):\n",
    "            whole_collection += parse_song_to_dict(i, full_path)\n",
    "        \n",
    "        i += 1\n",
    "        \n",
    "    whole_collection_df = pd.DataFrame(whole_collection)\n",
    "    \n",
    "    return whole_collection_df.astype({SEQUENCE_NUMBER: 'Int64', MEASURE_NUMBER: 'Int64', CHORD_NUMBER: 'Int64', \\\n",
    "                                      REPETITION: 'Int64'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_df = create_whole_collection_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BUGS: Point = répétition du même accord? A élucider et modifier. DONE\n",
    "# Il y a des mesures à un seul accord, d'autres à un accord par temps. A prendre en compte ! DONE\n",
    "#Introduire un sequence_id qui identifie un chorus, un verse, etc... DONE\n",
    "#Signalétique indiquant une répétition (x) ou un elid (->) à prendre en compte (colonne instrument) DONE\n",
    "#Dans certains morceaux, il y a des mesures au metre différent du metre principal, indiqué entre (). DONE\n",
    "#La lettre Z est parfois classée comme \"type\", parfois comme \"structure\" DONE\n",
    "\n",
    "#&pause est de taille arbitraire, pas forcément une mesure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now analyse some basic statistics on our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basic statistics: Ten most frequent chords in the corpus\n",
    "collection_df.groupby(CHORD)[CHORD].count().sort_values(ascending = False).head(10).plot.bar()\n",
    "plt.title('Ten most frequent chords in the corpus', fontsize = 20)\n",
    "plt.xlabel('Chord', fontsize = 18)\n",
    "plt.xticks(rotation = 'horizontal', fontsize = 9)\n",
    "plt.ylabel('Counts')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basic statistics: Number of unique chords per songs\n",
    "unique_chord_songs = collection_df[[SONG_ID,CHORD]].drop_duplicates().groupby(SONG_ID).count()\n",
    "n_bins = int(unique_chord_songs.max())+1\n",
    "unique_chord_songs.plot.hist(bins = n_bins, legend = False)\n",
    "plt.title('Distribution of the number of unique chords per song', fontsize = 20)\n",
    "plt.xlabel('Number of unique chord', fontsize = 18)\n",
    "plt.ylabel('Frequency', fontsize = 18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of songs that contain a chorus : %s\" %str(np.sum(collection_df[[SONG_ID,TYPE]].drop_duplicates()[TYPE] == \"chorus\")))\n",
    "\n",
    "print(\"Number of songs that contain a verse : %s\" %str(np.sum(collection_df[[SONG_ID,TYPE]].drop_duplicates()[TYPE] == \"verse\")))\n",
    "\n",
    "unique_section_songs = collection_df[[SONG_ID,TYPE]].drop_duplicates().groupby(SONG_ID)[TYPE].apply(list)\n",
    "print(\"Number of songs that contain both : %s\" %str(np.sum(unique_section_songs.apply(lambda l : \"chorus\" in l and \"verse\" in l))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
