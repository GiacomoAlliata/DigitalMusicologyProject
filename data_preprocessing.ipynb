{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestone 2 - Data gathering and preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Narrowing down the research question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Research Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this research project we will try to analyse a corpus of popular songs to try to identify  chords differences between verses and choruses. We will try to  answer the following research questions :\n",
    "<ol>\n",
    "<li>Does the chord distribution of the choruses differ from the one in the verses?</li>\n",
    "<li>Is there a different chord sequence distribution in the choruses compared to the verses?</li>\n",
    "<li>How are these distributions evolving over time? </li>\n",
    "</ol>\n",
    "\n",
    "In the first question we want to know if a difference exists in terms of chords statistic. Are we more likely to find a specific chord in the choruses? Are fewer different chords used in the chorus compared to the other parts of a song? <br> \n",
    "For the second one we want to have a more melodic insight. Can we find specific patterns? Is the Markov model derived from the chorus different from its counterparts? <br>\n",
    "In the third one the focus is on the time dimension. We want to know if the results of the two previous answers change over time. Were chorus closer to verses in 1968 than in 1985? How does each chord evolve in relation to the others? Do we find a sequence that appeared while other disappeared? \n",
    "\n",
    "These are all the underlying questions we want to answer under our main research questions.  \n",
    "\n",
    "These questions relate to our original idea to fully characterize a chorus over the years, especially in comparison with verses. We now want to focus specifically on the chords to differentiate these different parts, while keeping the temporal dimension as a potential factor to observe changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset presentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To give us the means to answer our questions, we have selected a dataset containing approximately 900 Pop-Rock songs in the top Billboard charts from the 60s to the 90s. They are simple text files with the following informations : \n",
    "* Release date\n",
    "* Song title \n",
    "* Artist name\n",
    "* Labels for the different parts of the song (such as chorus or verse)\n",
    "* Timestamp of each musical phrase beginning\n",
    "* Chords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the chorus/verse annotations to classify the chords in each group. This will allow us to divide the chords in two groups and compute statistics and distributions for each of them. As discussed in the research questions, we will start with a basic characterization, simply comparing which chords appear in which section. We will then move deeper and compare the distributions of chords as well as the Markov models. These will be computed at least with bigrams, perhaps with higher-n n-grams depending on the number of chords in each section (as it makes little sense to use n-grams with n close to the number of chords in a given section).\n",
    "\n",
    "The metadata, especially the release date, will be used to study the evolution over time of the previously discussed statistics. Depending on the distribution of songs over the years, time analysis will be discussed either over years or over decades."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Possible outcomes and confidence measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The different outcomes we can reasonably expect are: \n",
    "<ul>\n",
    "    <li> <strong>Null results:</strong> There are no significant differences between choruses and verses and no evolution over time. This could be explained by a bias in the corpus toward a specific Pop-Rock genre using the same chords all the time or maybe there is indeed no difference in the chords used in a chorus and the ones used in verses, which would constitute an answer for our questions. </li>\n",
    "<li> <strong>Narrow chorus chord distribution:</strong> Since the chorus has to be immediately recognized as one, maybe the composers make more extensive use of a sub group of the chords to ensure it. The same reasoning could be applied to the chord sequences: perhaps some specific ones will be more dominant in the chorus.</li>\n",
    "<li> <strong>Temporal evolution:</strong> It will be interesting to see if the differences between verse and chorus change over the years. This evolution, if present, could be linear or oscillating. A linear narrowing would imply that choruses are becoming more and more similar or verse more and more diverse. An oscillating pattern would be interesting as musical phenomenon could appear and disappear.</li>\n",
    "</ul>\n",
    "\n",
    "Statistical tests will be used throughtout our analysis to check if our findings are statistically relevant. Error bars will also be included in all our graphics to avoid wrong conclusions. Of course the relative small size of our corpus will influence our results but only further analysis can reveal if significant results can still be found."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Gathering the data\n",
    "The dataset has been created by [1] and corresponds to a random sample of 890 Billboard chart slots presented at ISMIR 2011 and MIREX 2012. Due to the nature of the sampling algorithm, there are some duplicates and this results in only 740 distinct songs. According to the authors, training algorithms that assume independent, identically distributed data should retain the duplicates.<br> This dataset is publicly available at https://ddmal.music.mcgill.ca/research/The_McGill_Billboard_Project_(Chord_Analysis_Dataset)/ and can be downloaded in various formats. Different features are given by the authors. In this project we will use metadata and chords annotations. \n",
    "The first dataset used is the index to the dataset (csv format), containing the following fields:\n",
    "<ul>\n",
    "<li><b>id</b>, the index for the sample entry.</li>\n",
    "<li><b>chart_date</b>, the date of the chart for the entry.</li>\n",
    "<li><b>target_rank</b>, the desired rank on that chart.</li>\n",
    "<li><b>actual_rank</b>, the rank of the song actually annotated, which may be up to 2 ranks higher or lower than the target rank [1, 2].</li>\n",
    "<li><b>title</b>, the title of the song annotated.</li>\n",
    "<li><b>artist</b>, the name of the artist performing the song annotated.</li>\n",
    "<li><b>peak_rank</b>, the highest rank the song annotated ever achieved on the Billboard Hot 100.</li>\n",
    "<li><b>weeks_on_chart</b>, the number of weeks the song annotated spent on the Billboard Hot 100 chart in total.</li>\n",
    "</ul>\n",
    "\n",
    "The main dataset comprehends chords, structure, instrumentation, and timing, given in a txt format. The annotation for each song begins with a header containing the title of the song, the name of the artist, the metre and the tonic pitch class of the opening key. In the main body, each line consists of a single phrase and begins with its timestamp, followed by the chords. This requires us to design a specific parser, as will be discussed in the next section.<br>\n",
    "We downloaded the two datasets which constitutes the whole of their database so we have the maximum from this source. It is not excluded that we find some additional ones to perform further analysis. As for now we will try to get as much stastitically relevant information from this source.\n",
    "\n",
    "[1]: John Ashley Burgoyne, Jonathan Wild, and Ichiro Fujinaga, ‘An Expert Ground Truth Set for Audio Chord Recognition and Music Analysis’, in Proceedings of the 12th International Society for Music Information Retrieval Conference, ed. Anssi Klapuri and Colby Leider (Miami, FL, 2011), pp. 633–38"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this question is to load the data and have a look at it. A specific parser is designed to do this automatically, in order to extract and store in a Pandas dataframe all the relevant informations and musical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "import cufflinks as cf\n",
    "cf.go_offline()\n",
    "cf.set_config_file(theme='white')\n",
    "import plotly.express as px\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_df = pd.read_csv(\"data/billboard-2.0-index.csv\")\n",
    "metadata_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('There are %d entries in the index table.' %len(metadata_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('There are %d entries with a given title.' %metadata_df.title.isna().value_counts()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('There are %d entries with a given artist.' %metadata_df.artist.isna().value_counts()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('There are %d entries with a given chart date.' %metadata_df.chart_date.isna().value_counts()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "months = {'01':'January', \n",
    "          '02':'February',\n",
    "          '03':'March',\n",
    "          '04':'April',\n",
    "          '05':'May',\n",
    "          '06':'June',\n",
    "          '07':'July',\n",
    "          '08':'August',\n",
    "          '09':'September',\n",
    "          '10':'October',\n",
    "          '11':'November',\n",
    "          '12':'December'}\n",
    "def format_date(date):\n",
    "    year = date[:4]\n",
    "    month = date[5:7]\n",
    "    day = date[-2:]\n",
    "    if day == '01':\n",
    "        suffix = 'st'\n",
    "    elif day == '02':\n",
    "        suffix = 'nd'\n",
    "    elif day == '03':\n",
    "        suffix = 'rd'\n",
    "    else:\n",
    "        suffix = 'th'\n",
    "        \n",
    "    if day[0] == '0':\n",
    "        day = day[1]\n",
    "    \n",
    "    date_string = months[month] + ' ' + day + suffix + ', ' + year\n",
    "    return(date_string)\n",
    "\n",
    "#Test\n",
    "format_date('1958-08-04')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The songs range from %s to %s.' %(format_date(metadata_df.chart_date.min()), format_date(metadata_df.chart_date.max())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parser "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SONG_ID, LINE_NUMBER, MEASURE_NUMBER, CHORD_NUMBER, SEQUENCE_NUMBER, \\\n",
    "CHORD, INSTRUMENT, TYPE, TIME, STRUCTURE, DURATION, REPETITION, ELID = \\\n",
    "\"song_id\",\"line_id\", \"measure_id\", \"chord_id\", \"sequence_id\",\\\n",
    "\"chord\", \"instrument\", \"section_type\", \"time\", \"section_structure\", \"duration\", \"repetition\", \"elided\"\n",
    "\n",
    "#This is dependant of \"metre\" in the txt files.\n",
    "METRE = \"metre\"\n",
    "\n",
    "#Create a new dictionary from two other\n",
    "def immutable_merge(dic1, dic2):\n",
    "    result = dic1.copy()\n",
    "    result.update(dic2)\n",
    "    return result\n",
    "\n",
    "#Create a row of the futur df as a dictionary\n",
    "def create_row(persistent_attributes, line_attributes,\n",
    "               measure_number = None, chord_number = None, chord = None, duration = None):\n",
    "    result = immutable_merge(persistent_attributes, line_attributes)\n",
    "    \n",
    "    if not (measure_number is None and measure_number is None and chord_number is None and duration is None):\n",
    "        result[MEASURE_NUMBER] = measure_number\n",
    "        result[CHORD_NUMBER] = chord_number\n",
    "        result[CHORD] = chord\n",
    "        result[DURATION] = duration\n",
    "    \n",
    "    return result\n",
    "\n",
    "#Generate the attributes of a given line and update the sequence counter\n",
    "def process_line_metadata(header, line_counter, old_line_attributes, sequence_counter, suffix = \"\"):\n",
    "    \n",
    "    result = {}\n",
    "    \n",
    "    #Suffix (main instrument, elid, repetition)\n",
    "    old_instrument = str(old_line_attributes.get(INSTRUMENT))\n",
    "    \n",
    "    for suffix in suffix.split(\", \"):\n",
    "        \n",
    "        suffix = suffix.strip()\n",
    "        \n",
    "        #Repetition\n",
    "        if re.match(\"^x\\d+$\",suffix):\n",
    "            result[REPETITION] = int(suffix[1])\n",
    "        \n",
    "        #Elid\n",
    "        elif suffix == \"->\":\n",
    "            result[ELID] = True\n",
    "\n",
    "        #Instrument\n",
    "        else:\n",
    "            ##New instrument\n",
    "            if len(suffix) > 0 and suffix != \"\\n\":\n",
    "                result[INSTRUMENT] = suffix.strip(\"\\n\").strip(\",\").strip()\n",
    "\n",
    "            ##Main instrument continued (experimental)\n",
    "            elif not old_instrument.endswith(\")\") and old_instrument.lower() not in [\"nan\",\"none\"] \\\n",
    "            and len(old_instrument)>0:\n",
    "                result[INSTRUMENT] = old_instrument.strip(\"(\")\n",
    "\n",
    "        \n",
    "    #Line number\n",
    "    result[LINE_NUMBER] = line_counter\n",
    "\n",
    "    \n",
    "    #Header    \n",
    "    header_items = header.split()\n",
    "        \n",
    "    result[TIME] = header_items[0]\n",
    "    \n",
    "    #Case where a section is continued\n",
    "    if len(header_items) == 1:\n",
    "        result[TYPE] = old_line_attributes.get(TYPE)\n",
    "        result[STRUCTURE] = old_line_attributes.get(STRUCTURE)\n",
    "        result[SEQUENCE_NUMBER] = old_line_attributes.get(SEQUENCE_NUMBER)\n",
    "    \n",
    "    #Case where a section has no structure (silence, end, fadeout)\n",
    "    elif len(header_items) == 2:\n",
    "        \n",
    "        #Z is a structure, not a type.\n",
    "        if header_items[1].strip().strip(\",\") == \"Z\":\n",
    "            result[STRUCTURE] = header_items[1].strip().strip(\",\")\n",
    "        else:\n",
    "            result[TYPE] = header_items[1].strip().strip(\",\")\n",
    "            \n",
    "        result[SEQUENCE_NUMBER] = sequence_counter\n",
    "        sequence_counter += 1\n",
    "    \n",
    "    #Case where a section begins.\n",
    "    elif len(header_items) == 3:\n",
    "        result[STRUCTURE] = header_items[1].strip().strip(\",\")\n",
    "        result[TYPE] = header_items[2].strip().strip(\",\")\n",
    "        result[SEQUENCE_NUMBER] = sequence_counter\n",
    "        sequence_counter += 1\n",
    "    \n",
    "    return sequence_counter, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_song_to_dict(song_id, path):\n",
    "    \n",
    "    rows = []\n",
    "    persistent_attributes = {}\n",
    "    \n",
    "    persistent_attributes[SONG_ID] = song_id\n",
    "    \n",
    "    with open(path,\"r\") as file:\n",
    "        line = file.readline()\n",
    "        \n",
    "        line_counter = 0\n",
    "        measure_counter = 0\n",
    "        chord_counter = 0\n",
    "        sequence_counter = 0\n",
    "        line_attributes = {}\n",
    "        old_chord = None\n",
    " \n",
    "\n",
    "        while line:\n",
    "        \n",
    "            if line != \"\\n\":\n",
    "\n",
    "                #Attribute lines\n",
    "                if line.startswith(\"#\"):\n",
    "                    attribute, value = line.strip(\"#\").split(\":\",1)\n",
    "                    persistent_attributes[attribute.strip(\" \")] = value.strip(\" \").strip(\"\\n\")\n",
    "\n",
    "                else:\n",
    "                    line_items = line.split(\"|\")\n",
    "\n",
    "                    #Special lines\n",
    "                    if len(line_items) <= 1:\n",
    "                        sequence_counter, line_attributes = \\\n",
    "                        process_line_metadata(line, line_counter, line_attributes, sequence_counter)\n",
    "                        row = create_row(persistent_attributes, line_attributes)\n",
    "                        rows.append(row)\n",
    "\n",
    "                    #Standard lines    \n",
    "                    else:                    \n",
    "                        header = line_items[0]\n",
    "                        suffix = line_items[-1]\n",
    "                        measures = line_items[1:-1]\n",
    "\n",
    "                        sequence_counter, line_attributes = \\\n",
    "                        process_line_metadata(header, line_counter, line_attributes, sequence_counter, suffix)  \n",
    "\n",
    "                        for measure in measures:\n",
    "                            \n",
    "                            chords = measure.split()\n",
    "                            \n",
    "                            #Special metric (experimental)\n",
    "                            old_metre = persistent_attributes.get(METRE)\n",
    "                            if re.match(\"^\\(\\d/\\d\\)$\", chords[0]):\n",
    "                                persistent_attributes[METRE] = str(chords[0][1]) + \"/\" + str(chords[0][3])\n",
    "                                chords = chords[1:]\n",
    "                            \n",
    "                            if len(chords) == 1:\n",
    "                                duration = \"measure\"\n",
    "                            elif len(chords) == 2 and persistent_attributes[METRE] in [\"4/4\",\"12/8\"]:\n",
    "                                duration = \"half-measure\"\n",
    "                            else:\n",
    "                                duration = \"beat\"\n",
    "                            \n",
    "                            for chord in chords:\n",
    "                                \n",
    "                                if chord == \".\":\n",
    "                                    chord = old_chord\n",
    "                                \n",
    "                                row = create_row(persistent_attributes, line_attributes,\n",
    "                                                 measure_counter, chord_counter, chord, duration)\n",
    "                                rows.append(row)\n",
    "                                old_chord = chord\n",
    "                                chord_counter += 1\n",
    "\n",
    "                            measure_counter += 1\n",
    "                            persistent_attributes[METRE] = old_metre\n",
    "            \n",
    "            #Finally\n",
    "            line_counter += 1\n",
    "            line = file.readline()\n",
    "    \n",
    "    \n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.DataFrame(parse_song_to_dict(0,\"data/McGill-Billboard/0004/salami_chords.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_whole_collection_df():\n",
    "    \n",
    "    path = \"data/McGill-Billboard/\"\n",
    "    file_name = \"/salami_chords.txt\"\n",
    "    UPPER_BOUND = 1300\n",
    "    \n",
    "    whole_collection = []\n",
    "    \n",
    "    i = 0\n",
    "    while i <= UPPER_BOUND:\n",
    "        full_path = path + \"0\"*(4-len(str(i)))+ str(i) + file_name\n",
    "        \n",
    "        if os.path.exists(full_path):\n",
    "            whole_collection += parse_song_to_dict(i, full_path)\n",
    "        \n",
    "        i += 1\n",
    "        \n",
    "    whole_collection_df = pd.DataFrame(whole_collection)\n",
    "    \n",
    "    return whole_collection_df.astype({SEQUENCE_NUMBER: 'Int64', MEASURE_NUMBER: 'Int64', CHORD_NUMBER: 'Int64', \\\n",
    "                                      REPETITION: 'Int64'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_df = create_whole_collection_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory statistical analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the number of sample per year we have, we see it's pretty well distributed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years=metadata_df.chart_date.map(lambda y:pd.to_datetime(y).year).value_counts(sort=False)\n",
    "years.iplot(kind='bar', title=\"Number of sample song per year\", xTitle=\"Year\", yTitle=\"Samples\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_df.chord.value_counts().head(24).iplot(kind='bar',title=\"Overall chord occurence ditribution\",xTitle=\"Chord\",yTitle=\"Occurence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_df[collection_df[\"section_type\"]==\"chorus\"].chord.value_counts().head(20).iplot(kind='bar',title=\"Chorus chord occurence ditribution\",xTitle=\"Chord\",yTitle=\"Occurence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of songs that contain a chorus : %s\" %str(np.sum(collection_df[[SONG_ID,TYPE]].drop_duplicates()[TYPE] == \"chorus\")))\n",
    "\n",
    "print(\"Number of songs that contain a verse : %s\" %str(np.sum(collection_df[[SONG_ID,TYPE]].drop_duplicates()[TYPE] == \"verse\")))\n",
    "\n",
    "unique_section_songs = collection_df[[SONG_ID,TYPE]].drop_duplicates().groupby(SONG_ID)[TYPE].apply(list)\n",
    "print(\"Number of songs that contain both : %s\" %str(np.sum(unique_section_songs.apply(lambda l : \"chorus\" in l and \"verse\" in l))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basic statistics: Number of unique chords per songs\n",
    "unique_chord_songs = collection_df[[SONG_ID,CHORD]].drop_duplicates().groupby(SONG_ID).count()\n",
    "n_bins = int(unique_chord_songs.max())+1\n",
    "unique_chord_songs.plot.hist(bins = n_bins, legend = False)\n",
    "plt.title('Distribution of the number of unique chords per song', fontsize = 20)\n",
    "plt.xlabel('Number of unique chord', fontsize = 18)\n",
    "plt.ylabel('Frequency', fontsize = 18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestone 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:1px solid black; padding:10px 10px; background-color: #FBE7C6;\"> \n",
    "    <div style = \"font-size:25px; text-align:center;\"><strong>Final processing</strong></div>\n",
    "    <div>\n",
    "        The first step for this milestone is to finish the processing of our dataset in order to obtain a dataframe containing all the chords of all songs that have both a verse and a chorus. We also add a parser for the chord notation used to better understand it. Thus we will:\n",
    "            <ul>\n",
    "                <li>parse the notation of the chords used in the dataset</li>\n",
    "                <li>implement the relative-to-tonic pitch class</li>\n",
    "                <li>discard all songs without both a verse and a chorus</li>\n",
    "                <li>split the dataframe to have a row per beat</li>\n",
    "                <li>create a function to squeeze all chords of a song in a list</li>\n",
    "            </ul>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:1px solid black; padding:10px 10px; background-color: #A0E7E5; text-align: center; font-size:20px;\"> \n",
    "    <strong>Parser for the chords</strong>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also define a parser to better handle the chords. This is based on the notation defined in http://ismir2005.ismir.net/proceedings/1080.pdf. Some notations are also added by the authors of the dataset and therefore included to handle all the 989 unique chords in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('There are %d unique chords in the dataset.' %len(collection_df.chord.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split a string and changes the target field depending on which side of the split to take\n",
    "#side : 0 for left and 1 for right\n",
    "def split_add(c,dic,split_char,target,side):\n",
    "    temp=str.split(str(c),split_char)\n",
    "\n",
    "    dic[target]=temp[side]\n",
    "    return dic, temp[1-side]\n",
    "\n",
    "\n",
    "def chord_to_tab(c):\n",
    "    c=str(c)\n",
    "    chord={\"root\":\"\", \"shorthand\" : \"\", \"degree_list\":[], \"bass\":\"\", \"N\" :False}\n",
    "    rest=\"\"\n",
    "    \n",
    "    if(c==\"N\"):\n",
    "        chord[\"N\"] = True\n",
    "        return chord\n",
    "    \n",
    "    c=c.replace(\")\",\"\")\n",
    "    \n",
    "    if('/' in c):\n",
    "        chord, rest=split_add(c,dic=chord,split_char=\"/\",target=\"bass\",side=1)\n",
    "    else :\n",
    "        rest=c\n",
    "    if(':' in rest):\n",
    "        chord, rest=split_add(rest,dic=chord,split_char=\":\",target=\"root\",side=0)\n",
    "    if('(' in rest):\n",
    "        chord, rest=split_add(rest,dic=chord,split_char=\"(\",target=\"degree_list\",side=1)\n",
    "    if(rest != \"\"):\n",
    "        chord[\"shorthand\"]=rest\n",
    "    \n",
    "    return chord\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_df['chord_dic'] = collection_df.chord.map(lambda y : chord_to_tab(y))\n",
    "for key in ['root', 'shorthand', 'degree_list', 'bass', 'N']:\n",
    "    collection_df[key] = collection_df.chord_dic.map(lambda y: y[key])\n",
    "collection_df.drop(columns = 'chord_dic', inplace = True)\n",
    "#df = collection_df[['song_id', 'title', 'artist', 'tonic', 'section_type', 'sequence_id', 'chord', 'root', 'shorthand', 'degree_list', 'bass', 'N']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:1px solid black; padding:10px 10px; background-color: #A0E7E5; text-align: center; font-size:20px;\"> \n",
    "    <strong>Creation of new columns fo relative-to-tonic roots</strong>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TPC_DIC = {\"Cb\":11,\"C\":0,\"C#\":1,\"Db\":1,\"D\":2,\"D#\":3,\"Eb\":3,\"E\":4,\"E#\":5,\"Fb\":4,\"F\":5,\"F#\":6,\n",
    "           \"Gb\":6,\"G\":7,\"G#\":8,\"Ab\":8,\"A\":9,\"A#\":10,\"Bb\":10,\"B\":11,\"B#\":0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_df[\"root_tpc\"] = collection_df.root.apply(lambda r : TPC_DIC.get(r))\n",
    "collection_df = collection_df.astype({\"root_tpc\":\"Int64\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_df[\"relative_root_tpc\"] =\\\n",
    "collection_df.apply(lambda row: (row[\"root_tpc\"] - TPC_DIC.get(row[\"tonic\"]))%12,axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_df = collection_df.fillna({REPETITION:1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:1px solid black; padding:10px 10px; background-color: #A0E7E5; text-align: center; font-size:20px;\"> \n",
    "    <strong>Creation of the dataframe with only songs that have both a verse and a chorus</strong>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_songs = collection_df[[SONG_ID,TYPE]].drop_duplicates().groupby(SONG_ID)[TYPE].apply(list)\\\n",
    ".apply(lambda l: \"chorus\" in l and \"verse\" in l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_collection_df = collection_df.merge(valid_songs.reset_index().rename(columns = {TYPE:\"valid\"}),on = SONG_ID)\n",
    "d_collection_df = d_collection_df[d_collection_df.valid].drop(columns = 'valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The corpus has %d songs.' %len(d_collection_df.song_id.unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:1px solid black; padding:10px 10px; background-color: #A0E7E5; text-align: center; font-size:20px;\"> \n",
    "    <strong>Creation of a one-row-per-beat dataframe</strong>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Duration of measure: all metre in the corpus are regular except song 700 (5/8 = 3 + 2). This was counted as two beats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_row(metre, duration, repetition):\n",
    "    \"\"\"\n",
    "    Indicate to how many beats in total a chord described on a dataframe's row correspond.\n",
    "    Careful ! The chords are non-successive in case of repetition\n",
    "    \"\"\"\n",
    "    num, denom = metre.split(\"/\")\n",
    "    \n",
    "    if metre == \"5/8\": #Specific to this dataset\n",
    "        beat_per_measure = 2\n",
    "    else:\n",
    "        beat_per_measure = int(num)/(1 if int(denom) == 4 else 3)\n",
    "    \n",
    "    if duration == \"measure\":\n",
    "        return beat_per_measure*(repetition)\n",
    "    \n",
    "    elif duration == \"half-measure\":\n",
    "        return beat_per_measure/2*(repetition)\n",
    "    \n",
    "    elif duration == \"beat\":\n",
    "        return repetition\n",
    "    \n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def successive_repetition_row(metre,duration):\n",
    "    \"\"\"\n",
    "    Indicate on how many successive beats a chord is present\n",
    "    \"\"\"\n",
    "    return weight_row(metre,duration,repetition=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of one-row-per-bit dataframe\n",
    "N_SUCC_BEATS = \"n_succ_beats\"\n",
    "\n",
    "d_collection_df[N_SUCC_BEATS] = d_collection_df.apply(\\\n",
    "    lambda row: successive_repetition_row(row[METRE], row[DURATION]),axis=1)\n",
    "d_collection_df = d_collection_df.astype({N_SUCC_BEATS:\"Int64\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def create_beats_df(d_collection_df):\n",
    "    beats_dics = []\n",
    "    repetition_flag = False\n",
    "    repeted_dics = []\n",
    "    repetition_line = np.PINF\n",
    "    repetition_song = np.PINF\n",
    "    for i in tqdm(d_collection_df.reset_index().index):\n",
    "\n",
    "        if repetition_flag == True and\\\n",
    "(repetition_line != d_collection_df.iloc[i][LINE_NUMBER] or repetition_song != d_collection_df.iloc[i][SONG_ID]):\n",
    "\n",
    "            for r in range(repetition_n):\n",
    "                beats_dics += repeted_dics\n",
    "\n",
    "            repetition_flag = False\n",
    "            repeted_dics = []\n",
    "            repetition_line = np.PINF\n",
    "            repetition_song = np.PINF\n",
    "        \n",
    "        \n",
    "        if d_collection_df.iloc[i][REPETITION] == 1 :\n",
    "\n",
    "            for b in range(d_collection_df.iloc[i][N_SUCC_BEATS]):\n",
    "                beats_dics.append(d_collection_df.iloc[i].to_dict())\n",
    "\n",
    "        else:\n",
    "\n",
    "            repetition_flag = True\n",
    "            repetition_line = d_collection_df.iloc[i][LINE_NUMBER]\n",
    "            repetition_song = d_collection_df.iloc[i][SONG_ID]\n",
    "            repetition_n = d_collection_df.iloc[i][REPETITION]\n",
    "\n",
    "            for b in range(d_collection_df.iloc[i][N_SUCC_BEATS]):\n",
    "                repeted_dics.append(d_collection_df.iloc[i].to_dict())\n",
    "\n",
    "    beats_collection_df = pd.DataFrame(beats_dics)\n",
    "    \n",
    "    return beats_collection_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beats_collection_df = create_beats_df(d_collection_df)\n",
    "beats_collection_df = beats_collection_df.drop(N_SUCC_BEATS,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:1px solid black; padding:10px 10px; background-color: #FFAEBC;\">\n",
    "    <strong>Conclusion:</strong> the final dataframe beats_collection_df has the following properties:\n",
    "    <ul>\n",
    "        <li>each row corresponds to a beat and thus to a chord played</li>\n",
    "        <li>only songs with both a verse and a chorus are kept</li>\n",
    "        <li>all elements of a chord (root, quality, degree list and bass note) are given</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:1px solid black; padding:10px 10px; background-color: #A0E7E5; text-align: center; font-size:20px;\"> \n",
    "    <strong>Squeeze function</strong>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a last processing step, we define a function to obtain a dataframe with all the chords of a song squeezed in its various sections (intro, first verse, chorus, second verse...). This will be helpful later in our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The squeeze function returns a dataframe with the all the chords of a song squeezed in a row dependent on a subgroup\n",
    "# of the section type. Default is \"none\" and will not filter any type of songs.\n",
    "#try subgroup=\"chorus\" or subgroup=\"verse\"\n",
    "\n",
    "def compress(s) :\n",
    "    return s.dropna().to_list()\n",
    "\n",
    "def squeeze(df,subgroup=\"none\"):\n",
    "    df_local=df\n",
    "    if(subgroup!=\"none\"):\n",
    "        df_local=df[df[\"section_type\"]==subgroup]\n",
    "    \n",
    "    chords = df_local.groupby([\"song_id\",\"title\"]).chord.agg(compress).reset_index()[['song_id', 'chord']]\n",
    "    roots = df_local.groupby([\"song_id\",\"title\"]).root.agg(compress).reset_index()[['song_id', 'root']]\n",
    "    shorthands = df_local.groupby([\"song_id\",\"title\"]).shorthand.agg(compress).reset_index()[['song_id', 'shorthand']]\n",
    "    \n",
    "    return chords.merge(roots, on = 'song_id').merge(shorthands, on = 'song_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squeeze(collection_df).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#squeeze section allows you to squeeze the sections of songs\n",
    "def squeeze_section(df,subgroup=\"none\"):\n",
    "    df_local=df\n",
    "    if(subgroup!=\"none\"):\n",
    "        df_local=df[df[\"section_type\"]==subgroup]\n",
    "    \n",
    "    chords = df_local.groupby([\"song_id\",\"sequence_id\",\"section_type\"]).chord.agg(compress).reset_index()\n",
    "    roots = df_local.groupby([\"song_id\",\"sequence_id\",\"section_type\"]).root.agg(compress).reset_index()\n",
    "    shorthands = df_local.groupby([\"song_id\",\"sequence_id\",\"section_type\"]).shorthand.agg(compress).reset_index()\n",
    "    \n",
    "    return chords.merge(roots, on = ['song_id', 'sequence_id', 'section_type']).merge(shorthands, on = ['song_id', 'sequence_id', 'section_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squeeze_section(collection_df).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now move to the exploratory analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:1px solid black; padding:10px 10px; background-color: #FBE7C6;\"> \n",
    "    <div style = \"font-size:25px; text-align:center;\"><strong>Chord distribution</strong></div>\n",
    "    <div>\n",
    "        The first axe of exploration is the most basic one: simply counting. In this section we will look at:\n",
    "            <ul>\n",
    "                <li>the most common chords used in the songs</li>\n",
    "                <li>the distribution of unique chords in the songs</li>\n",
    "                <li>the roots of the chords</li>\n",
    "                <li>the quality of the chords</li>\n",
    "                <li>the bass notes of the chords</li>\n",
    "                <li>the Bag of Words representation of the songs</li>\n",
    "            </ul>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:1px solid black; padding:10px 10px; background-color: #A0E7E5; text-align: center; font-size:20px;\"> \n",
    "    <strong>Most common chords used</strong>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can start by finding the most used chords in a given section. By default we look at the 50 most common chords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chord_distribution(df, section = 'verse', top = 50):\n",
    "    title = 'Chord distribution in the ' + section\n",
    "    plt.figure(figsize = (16,6))\n",
    "    df[df.section_type == section].chord.value_counts().iloc[:top].plot(kind='bar')\n",
    "    plt.title(title, fontsize = 20)\n",
    "    plt.xlabel('Chord', fontsize = 18)\n",
    "    plt.xticks(rotation = 50)\n",
    "    plt.ylabel('Number of chords', fontsize = 18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chord_distribution(beats_collection_df, 'verse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chord_distribution(beats_collection_df, 'chorus')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:1px solid black; padding:10px 10px; background-color: #A0E7E5; text-align: center; font-size:20px;\"> \n",
    "    <strong>Distribution of unique chords</strong>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at the distribution of unique chords used in songs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetUniqueChords(df):\n",
    "    return df.groupby('song_id').chord.unique().map(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_chords_section(df, section = 'verse'):\n",
    "    unique_chords = GetUniqueChords(df[df.section_type == section])\n",
    "    n_bins = int(unique_chords.max()) + 1\n",
    "    unique_chords.plot.hist(bins = n_bins, legend = False)\n",
    "    title = 'Distribution of the number of unique chords per song in the ' + section\n",
    "    plt.title(title, fontsize = 20)\n",
    "    plt.xlabel('Number of unique chords used', fontsize = 18)\n",
    "    plt.ylabel('Frequency', fontsize = 18)\n",
    "    plt.show()\n",
    "    \n",
    "    print('Statistics:')\n",
    "    print(unique_chords.describe())\n",
    "    \n",
    "    return (unique_chords.describe()['mean'], unique_chords.describe()['std']/np.sqrt(unique_chords.describe()['count']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_verse = unique_chords_section(beats_collection_df, 'verse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_chorus = unique_chords_section(beats_collection_df, 'chorus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [beats_collection_df[beats_collection_df.section_type == 'verse'].groupby('song_id').count().title,\n",
    "        beats_collection_df[beats_collection_df.section_type == 'chorus'].groupby('song_id').count().title]\n",
    "plt.figure(figsize = (16,9))\n",
    "plt.boxplot(data, notch = True, \n",
    "                  bootstrap = 5000, \n",
    "                  showfliers = True, \n",
    "                  showmeans = True)\n",
    "plt.title('Number of unique chords used in each section', fontsize = 20)\n",
    "plt.xticks([1,2], ['Verse', 'Chorus'], fontsize = 14)\n",
    "plt.yticks(fontsize = 14)\n",
    "plt.xlabel('Section of the song', fontsize = 18)\n",
    "plt.ylabel('Number of unique chords used', fontsize = 18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll use statistical tests to find if the differences observed are significant. From the previous plots, we can already expect to not have significant differences here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import researchpy as rp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will run an independent t-test to check if the two sample data, i.e. the number of unique chords used in the verse and in the chorus sections of the songs, are the same or not. But first we need to check the independent t-test assumptions:\n",
    "   * homogeneity of variances (with the Levene's test)\n",
    "   * normal distributions of redisuals (with Shapiro-Wilk's test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.levene(data[0], data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.shapiro(data[0] - data[1]) #Need to have the same size, ie only keep songs with both a chorus & a verse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since both assumptions are met (both p-values are larger than 0.05, meaning the sample data do not violate the assumptions), we can proceed with the independent t-test. Let's start by having a quick look at some basic statistics (where the first line is for verse, the second for chorus and the third for the combined data). As already seen with the plots, means and SDs are nearly the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptives, results = rp.ttest(data[0], data[1])\n",
    "descriptives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the two sample data do not violate the null hypothesis because p-value=0.8502 > 0.05 (i.e. there are no significant differences)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:1px solid black; padding:10px 10px; background-color: #FFAEBC;\">\n",
    "    <strong>Conclusion:</strong> the distribution of the number of unique chords used in each section has ben investigated. For both verse and chorus sections, the distribution is right skewed with a mean slighly under 6 unique chords used and a tail with maximum values reaching up to 30 unique chords used. An independent t-test has been conducted to find if the differences are statistically significant. As expected, results (t[1595]=0.1889, p-value=0.8502 > 0.5) show that the two sample data do not violate the null hypothesis (i.e. the two distributions are similar). \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:1px solid black; padding:10px 10px; background-color: #A0E7E5; text-align: center; font-size:20px;\"> \n",
    "    <strong>Root of the chords</strong>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can look at the roots of the chords, first on the whole songs and then on each section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dist = beats_collection_df.root.value_counts()\n",
    "#We remove the missing values (marked with an empty space by our parser)\n",
    "root_dist.drop('', inplace = True)\n",
    "#Plot\n",
    "plt.figure(figsize = (16,5))\n",
    "root_dist.map(lambda x: 100 * x / root_dist.sum()).plot(kind = 'bar')\n",
    "plt.title('Root distribution of the chords in the whole songs (ordered)', fontsize = 20)\n",
    "plt.xlabel('Root', fontsize = 18)\n",
    "plt.xticks(rotation = 'horizontal')\n",
    "plt.ylabel('Proportion [%]', fontsize = 18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also move to the line of fifths representation to check if we get the characteristic bell shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dist = beats_collection_df.root.value_counts()\n",
    "#Let's move to the line of fifths\n",
    "fifths_line = ['B#', 'E#', 'A#', 'D#', 'G#', 'C#', 'F#', \n",
    "               'B', 'E', 'A', 'D', 'G', 'C', 'F', \n",
    "               'Bb', 'Eb', 'Ab', 'Db', 'Gb', 'Cb', 'Fb']\n",
    "root_dist = pd.Series(index = fifths_line, data = root_dist).fillna(0)\n",
    "#Plot\n",
    "plt.figure(figsize = (16,5))\n",
    "root_dist.map(lambda x: 100 * x / root_dist.sum()).plot(kind = 'bar')\n",
    "plt.title('Root distribution of the chords in the whole songs (line of fifths)', fontsize = 20)\n",
    "plt.xlabel('Root', fontsize = 18)\n",
    "plt.xticks(rotation = 'horizontal')\n",
    "plt.ylabel('Proportion [%]', fontsize = 18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def root_distribution_section(df, section = 'verse', root_repr = 'root', index = 1, fifths = True):\n",
    "    root_dist = df[df.section_type == section][root_repr].value_counts()\n",
    "    if fifths:\n",
    "        root_dist = pd.Series(index = fifths_line, data = root_dist).fillna(0)\n",
    "    else:\n",
    "        #We remove the non-roots notation elements\n",
    "        if '' in root_dist.index:\n",
    "            root_dist.drop('', inplace = True) \n",
    "\n",
    "    #Plot\n",
    "    plt.figure(figsize = (16,9))\n",
    "    plt.subplot(2, 1, index)\n",
    "    root_dist.map(lambda x: 100 * x / root_dist.sum()).plot(kind = 'bar')\n",
    "    title = 'Root distribution of the chords in the ' + section\n",
    "    if fifths:\n",
    "        title += ' (line of fifths)'\n",
    "    else:\n",
    "        title += ' (ordered)'\n",
    "    plt.title(title, fontsize = 20)\n",
    "    plt.xlabel('Root', fontsize = 18)\n",
    "    plt.xticks(rotation = 'horizontal')\n",
    "    plt.ylabel('Proportion [%]', fontsize = 18)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first plot the results on a normal scale from the most used to the least used roots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_distribution_section(beats_collection_df, 'verse', 'root', 1, False)\n",
    "root_distribution_section(beats_collection_df, 'chorus', 'root', 2, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then using again the line of fifths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_distribution_section(beats_collection_df, 'verse', 'root', 1)\n",
    "root_distribution_section(beats_collection_df, 'chorus', 'root', 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can also show the relative-to-the-tonic roots distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_distribution_section(beats_collection_df, 'verse', 'relative_root_tpc', 1, False)\n",
    "root_distribution_section(beats_collection_df, 'chorus', 'relative_root_tpc', 2, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:1px solid black; padding:10px 10px; background-color: #FFAEBC;\">\n",
    "    <strong>Conclusion:</strong> the distributions of roots (in either versions, relative or not) look fairly similar, with differences in the order of 1%. It is interesting to note that, when using the line of fifths order, we find the characteristic bell-shaped distribution. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:1px solid black; padding:10px 10px; background-color: #A0E7E5; text-align: center; font-size:20px;\"> \n",
    "    <strong>Quality of the chords</strong>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we can investigate the quality (or type) of the chords, using the parser previously defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quality_dist = beats_collection_df.shorthand.value_counts()\n",
    "#We remove the missing values (marked with an empty space by our parser)\n",
    "quality_dist.drop(['', '*', '&pause'], inplace = True)\n",
    "#Plot\n",
    "plt.figure(figsize = (16,5))\n",
    "quality_dist.map(lambda x: 100 * x / quality_dist.sum()).plot(kind = 'bar')\n",
    "plt.title('Quality distribution of the chords in the whole songs', fontsize = 20)\n",
    "plt.xlabel('Quality', fontsize = 18)\n",
    "plt.xticks(rotation = 45, fontsize = 14)\n",
    "plt.ylabel('Proportion [%]', fontsize = 18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quality_distribution_section(df, section = 'verse'):\n",
    "    quality_dist = df[df.section_type == section].shorthand.value_counts()\n",
    "    #We remove the non-roots notation elements\n",
    "    extra_notations = ['', '*', '&pause']\n",
    "    for notation in extra_notations:\n",
    "        if notation in quality_dist.index:\n",
    "            quality_dist.drop(notation, inplace = True) \n",
    "\n",
    "    #Plot\n",
    "    plt.figure(figsize = (16,9))\n",
    "    plt.subplot(2, 1, 1)\n",
    "    quality_dist.map(lambda x: 100 * x / quality_dist.sum()).plot(kind = 'bar')\n",
    "    title = 'Quality distribution of the chords in the ' + section\n",
    "    plt.title(title, fontsize = 20)\n",
    "    plt.xlabel('Quality', fontsize = 18)\n",
    "    plt.xticks(rotation = 45, fontsize = 14)\n",
    "    plt.ylabel('Proportion [%]', fontsize = 18)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quality_distribution_section(beats_collection_df, 'verse')\n",
    "quality_distribution_section(beats_collection_df, 'chorus')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that major chords are the most used by a large margin. Then come in a close range minor, seventh and minor seventh chords. All other chords have much lower frequencies of usage in both verse and chorus sections. Let's compare the mean proportion of these four predominants chord qualities between the two sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetCountsOfQuality(df, section, quality):\n",
    "    return df[df.section_type == section][df.shorthand == quality].groupby('song_id').count().title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['Major in verse', 'Major in chorus',\n",
    "          'Minor in verse', 'Minor in chorus',\n",
    "          'Seventh in verse', 'Seventh in chorus',\n",
    "          'Minor Seventh in verse', 'Minor Seventh in chorus']\n",
    "data = [GetCountsOfQuality(beats_collection_df, 'verse', 'maj'), \n",
    "        GetCountsOfQuality(beats_collection_df, 'chorus', 'maj'),\n",
    "        GetCountsOfQuality(beats_collection_df, 'verse', 'min'),\n",
    "        GetCountsOfQuality(beats_collection_df, 'chorus', 'min'),\n",
    "        GetCountsOfQuality(beats_collection_df, 'verse', '7'),\n",
    "        GetCountsOfQuality(beats_collection_df, 'chorus', '7'),\n",
    "        GetCountsOfQuality(beats_collection_df, 'verse', 'min7'),\n",
    "        GetCountsOfQuality(beats_collection_df, 'chorus', 'min7')]\n",
    "plt.figure(figsize = (16, 9))\n",
    "plt.boxplot(data,\n",
    "            notch = True,\n",
    "            bootstrap = 5000,\n",
    "            showfliers = False, #Don't show outliers to better compare medians between the two sections\n",
    "            showmeans = True)\n",
    "plt.xticks(range(1,9), labels, rotation = 14)\n",
    "plt.title('Boxplots of frequencies of usage of the four most common chord qualities, per section', fontsize = 20)\n",
    "plt.xlabel('Chord qualities and section', fontsize = 18)\n",
    "plt.ylabel('Frequency of usage', fontsize = 18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:1px solid black; padding:10px 10px; background-color: #FFAEBC;\">\n",
    "    <strong>Conclusion:</strong> the most striking result is certainly the overwhelming presence of major chords in the corpus, representing more than half of all the chords (followed by minor chords around 15%). However the differences between verse and chorus chords are fairly small again. We plot the four most common chord qualities (major, minor, seventh and minor seventh), representing more than 80% of our corpus, using boxplots to better compare between the two sections. According to the notches on the boxplots, some differences might indeed be statistically significant but further statistical tests would be needed to verity that.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:1px solid black; padding:10px 10px; background-color: #A0E7E5; text-align: center; font-size:20px;\"> \n",
    "    <strong>Bass note of the chords</strong>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the third element of a chord that we can observe is the bass note used (or the lack of it, in most cases)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bass_note_dist = beats_collection_df.bass.value_counts().rename({'':'No bass note'})\n",
    "#Plot\n",
    "plt.figure(figsize = (16,5))\n",
    "bass_note_dist.map(lambda x: 100 * x / bass_note_dist.sum()).plot(kind = 'bar')\n",
    "plt.title('Bass note distribution of the chords in the whole songs', fontsize = 20)\n",
    "plt.xlabel('Bass note', fontsize = 18)\n",
    "plt.xticks(rotation = 'horizontal')\n",
    "plt.ylabel('Proportion [%]', fontsize = 18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, most chords do not use a bass note. We will therefore exclude this case in order to better observe which notes are more commonly used as bass notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bassnote_distribution_section(df, section = 'verse'):\n",
    "    bass_note_dist = df[df.section_type == section].bass.value_counts()\n",
    "    #We remove the non-roots notation elements\n",
    "    if '' in bass_note_dist.index:\n",
    "        bass_note_dist.drop('', inplace = True) \n",
    "\n",
    "    #Plot\n",
    "    plt.figure(figsize = (16,9))\n",
    "    plt.subplot(2, 1, 1)\n",
    "    bass_note_dist.map(lambda x: 100 * x / bass_note_dist.sum()).plot(kind = 'bar')\n",
    "    title = 'Bass note distribution of the chords in the ' + section\n",
    "    plt.title(title, fontsize = 20)\n",
    "    plt.xlabel('Bass note', fontsize = 18)\n",
    "    plt.xticks(rotation = 'horizontal')\n",
    "    plt.ylabel('Proportion [%]', fontsize = 18)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bassnote_distribution_section(beats_collection_df, 'verse')\n",
    "bassnote_distribution_section(beats_collection_df, 'chorus')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we did for chord qualities, we will investigate more in depth the frequencies of usage of the most common bass notes, in this case note 5 and note 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetCountsOfBassNote(df, section, bass_note):\n",
    "    return df[df.section_type == section][df.bass == bass_note].groupby('song_id').count().title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['Note 5 in verse', 'Note 5 in chorus',\n",
    "          'Note 3 in verse', 'Note 3 in chorus']\n",
    "data = [GetCountsOfBassNote(beats_collection_df, 'verse', '5'), \n",
    "        GetCountsOfBassNote(beats_collection_df, 'chorus', '5'),\n",
    "        GetCountsOfBassNote(beats_collection_df, 'verse', '3'),\n",
    "        GetCountsOfBassNote(beats_collection_df, 'chorus', '3')]\n",
    "plt.figure(figsize = (16, 9))\n",
    "plt.boxplot(data,\n",
    "            notch = True,\n",
    "            bootstrap = 5000,\n",
    "            showfliers = True, #Here the outliers do not skew too much the boxplots\n",
    "            showmeans = True)\n",
    "plt.xticks(range(1,5), labels, rotation = 14)\n",
    "plt.title('Boxplots of frequencies of usage of the two most common bass notes, per section', fontsize = 20)\n",
    "plt.xlabel('Bass note and section', fontsize = 18)\n",
    "plt.ylabel('Frequency of usage', fontsize = 18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:1px solid black; padding:10px 10px; background-color: #FFAEBC;\">\n",
    "    <strong>Conclusion:</strong> As expected, most chords simply do not have a bass note. It seems this is more of an exception. Here, there seem to be some differences between the verse and the chorus, and again we use boxplots to better investigate the two most common bass notes used (notes 5 and 3). According to the notches, the differences don't seem significant but further tests can here again be used to assessed that.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:1px solid black; padding:10px 10px; background-color: #A0E7E5; text-align: center; font-size:20px;\"> \n",
    "    <strong>BoW representation</strong>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we try to move to a more advanced analysis, trying to analyze and compare songs and sections of songs as a whole. Let's start by creating a function to compute the Bag of Words representation of each song (or part of a song). In this case, the words will be all the possible chords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = dict.fromkeys(beats_collection_df.chord.unique(), 0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remove the non-chords elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for non_chord in ['', '&pause', 'nan']:\n",
    "    vocab.pop(non_chord, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createBOW(chord_list):\n",
    "    bow = vocab\n",
    "    for chord in chord_list:\n",
    "        if chord in vocab.keys():\n",
    "            bow[chord] += 1\n",
    "    #Normalise the BoW to get a distribution\n",
    "    total = sum(bow.values())\n",
    "    bow = {chord: count/total for chord, count in bow.items()}\n",
    "    return np.array(list(bow.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then start to compute the BoW representation of the whole songs, using the function squeeze() previously created to help us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bow = squeeze(collection_df)\n",
    "df_bow['bow'] = df_bow.chord.map(lambda x: createBOW(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bow.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check each bow value is indeed a distribution (ie the sum of the elements is 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bow.bow.map(lambda x: sum(x)).sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will do the same for the chorus and the verse section of songs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bow_verse = squeeze(collection_df, 'verse')\n",
    "df_bow_verse['bow'] = df_bow_verse.chord.map(lambda x: createBOW(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bow_verse.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bow_chorus = squeeze(collection_df, 'chorus')\n",
    "df_bow_chorus['bow'] = df_bow_chorus.chord.map(lambda x: createBOW(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bow_chorus.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now compare songs and sections of songs between them. The first idea is to compute the mean distance between all BoW representations of a section and compare it to the mean distance between all BoW representations of whole songs. Our assumption is that, if chorus and verse have specific characteristics, they should be rather similar from songs to songs and therefore have a smaller mean distance compared to the one of whole songs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ComputeMeanDistance(df, distance):\n",
    "    dist_sum = 0\n",
    "    count = 0\n",
    "    for i in range(len(df)):\n",
    "        for j in range(i):\n",
    "                dist_sum += distance(df.bow.iloc[i], df.bow.iloc[j])\n",
    "                count += 1\n",
    "    return dist_sum / count                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ComputeMeanDistance(df_bow, distance.euclidean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ComputeMeanDistance(df_bow_verse, distance.euclidean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ComputeMeanDistance(df_bow_chorus, distance.euclidean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:1px solid black; padding:10px 10px; background-color: #FFAEBC;\">\n",
    "    <strong>Conclusion:</strong> Using the Euclidean distance, we have found that indeed the mean distance of verse sections and chorus sections is smaller than the one of whole songs. Of course further tests would be needed to assert that the differences are statistically significant but this is a promising start and could be further explored. Other distance measures and similarities could be tested. In a similar way, we could investigate whether chords used in chorus sections are a subset of chords used in verse sections (using the Jaccard similarity for instance).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:1px solid black; padding:10px 10px; background-color: #FBE7C6;\"> \n",
    "    <div style = \"font-size:25px; text-align:center;\"><strong>Markov models</strong></div>\n",
    "    <div>\n",
    "        The second axe of exploration we have tried is a Markov model approach. In this section we will:\n",
    "            <ul>\n",
    "                <li>create the Markov models (for whole songs or only specific sections)</li>\n",
    "                <li>test the Markov models with differents basic elements (the chords, the roots of the chords, the quality of the chords</li>\n",
    "                <li>compare them</li>\n",
    "            </ul>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Circle of fifth\n",
    "\n",
    "fifth=[\"F\",\"C\",\"G\",\"D\",\"A\",\"E\",\"B\"]\n",
    "big_fifth=[\"F\",\"C\",\"G\",\"D\",\"A\",\"E\",\"B\"]\n",
    "\n",
    "for i in range(0,len(fifth)) :\n",
    "    \n",
    "    big_fifth.append(fifth[i]+\"#\")\n",
    "    big_fifth.insert(i,fifth[i]+\"b\")\n",
    "    \n",
    "######\n",
    "# Compute bigrams\n",
    "\n",
    "def bigrams_seq(seq):\n",
    "    l=list(zip(seq[:-1], seq[1:]))\n",
    "    return l\n",
    "\n",
    "\n",
    "def bigrams_corpus(seqs):\n",
    "    return [bg for seq in seqs for bg in bigrams_seq(seq)]\n",
    "\n",
    "\n",
    "def markov_heatmap(bigram_seq, order=big_fifth, display_N=False, hide_diag=False) :\n",
    "    \"\"\"Take as intput a sequence of bigram in a counter type element and returns a heatmap\n",
    "    order : display the axis in the given order, default is the circle of fifth\n",
    "    display_N: allows you to display the silences as well noted \"\"\n",
    "    hide_diag: don't display identical bigrams if true, default is False \"\"\"\n",
    "    #test_count=Counter(bigram_seq)\n",
    "    test_count=bigram_seq\n",
    "    \n",
    "    if(order=='none'):\n",
    "        x1s    = [ x1  for ((x1, x2), count) in test_count.items() ]\n",
    "        x2s    = [ x2  for ((x1, x2), count) in test_count.items() ]\n",
    "        y=pd.Series(x2s).drop_duplicates().sort_values()\n",
    "        x=pd.Series(x1s).drop_duplicates().sort_values(ascending=False)\n",
    "    else:\n",
    "        if(display_N == False):\n",
    "            y=order\n",
    "            x=reversed(order)\n",
    "            #x=order\n",
    "            \n",
    "    #df_local=df_store\n",
    "    df_local=pd.DataFrame(index=x,columns=y)\n",
    "    \n",
    "    #if(display_N==False):\n",
    "        #df_local=df_local.drop(index='')\n",
    "        #df_local=df_local.drop(columns='')\n",
    "    \n",
    "    for ((x1, x2), count) in test_count.items():\n",
    "        if(x1 != \"\" and x2 != \"\"):\n",
    "            df_local[x1][x2]=count\n",
    "            if(x1==x2 and hide_diag):\n",
    "                df_local[x1][x2]=0\n",
    "        elif(display_N==True):\n",
    "            df_local[x1][x2]=count\n",
    "\n",
    "            \n",
    "\n",
    "    df_local=df_local.fillna(0)\n",
    "    plt.figure(figsize=(7,7))\n",
    "    #print(df_local)\n",
    "    #return df_local\n",
    "    sns.heatmap(data=df_local,cmap=\"coolwarm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def substract_counts(c1,c2):\n",
    "    \"\"\"returns the substracted counters c1-c2\"\"\"\n",
    "    c=Counter()\n",
    "    for ((x1,x2),count) in c1.items() :\n",
    "        c[(x1,x2)]=c1[(x1,x2)]-c2[(x1,x2)]\n",
    "    return c    \n",
    "    \n",
    "    \n",
    "def add_count(y,count):\n",
    "    count+=Counter(y)\n",
    "    return\n",
    "    \n",
    "    \n",
    "def top_transition(c, n = 15, remove_equal = True, position = \"head\"):   \n",
    "    \"\"\"Plots the graph of the n most significant transition\n",
    "    c: counter element.\n",
    "    n: Number of elements to display \n",
    "    remove_equal: If True removes the transitions of two identical element\n",
    "    position: Gives the head or tail elements\"\"\"\n",
    "    \n",
    "    for ((x1,x2),count) in c.items() :\n",
    "        if(x1==x2 and remove_equal):\n",
    "               c[(x1,x2)]=0\n",
    "    \n",
    "    x1s = [ (x1,x2)  for ((x1, x2), count) in c.items() ]\n",
    "    values = [ count  for ((x1, x2), count) in c.items() ]\n",
    "    \n",
    "    \n",
    "    df=pd.DataFrame(data=values, index=x1s)\n",
    "    df=df.reset_index().set_index(0).sort_index(ascending=False).reset_index().set_index('index')\n",
    "    df=df.rename(columns={0:\"%\"},index={\"index\":\"bigrams\"})\n",
    "    \n",
    "    #Plot\n",
    "    title = 'Most frequent transitions'\n",
    "    if(position != \"tail\"):\n",
    "        df.head(n).plot(kind='bar', legend = False, figsize = (16,6))\n",
    "    else:\n",
    "        df.tail(n).plot(kind='bar', legend = False, figsize = (16,6))\n",
    "        title += ' (tail)'\n",
    "    plt.title(title, fontsize = 20)\n",
    "    plt.xticks(rotation = 45, fontsize = 12)\n",
    "    plt.xlabel('Transitions', fontsize = 18)\n",
    "    plt.ylabel('Occurences [%]', fontsize = 18)\n",
    "    plt.show()\n",
    "    \n",
    "    return \n",
    "    \n",
    "    \n",
    "def normalize_counter(c, remove_equal=True):\n",
    "    \n",
    "    tot=0\n",
    "    for ((x1,x2),count) in c.items() :\n",
    "        if(remove_equal and x1==x2 ):\n",
    "            tot+=0\n",
    "        else :\n",
    "            tot+=count\n",
    "    \n",
    "        \n",
    "    if(tot==0):\n",
    "        return \"error : empty counter\"\n",
    "    \n",
    "    for ((x1,x2),count) in c.items() :\n",
    "        c[(x1,x2)]=(c[(x1,x2)]*100)/tot\n",
    "    return\n",
    "        \n",
    "    \n",
    "    \n",
    "def count_bigrams(df, chord_representation = 'chord', subsection='none', normalize=False):\n",
    "    \"\"\"Counts the bigrams of a given given data frame \"df\" grouped relative to the subsection.\n",
    "    if normalize is true the output will be normalized (useful for comparing results)\"\"\"\n",
    "    \n",
    "    df = squeeze(df, subsection)\n",
    "    \n",
    "    df[\"bigrams\"] = df[chord_representation].map(lambda y : bigrams_seq(y))\n",
    "    c = Counter()\n",
    "   \n",
    "    df.bigrams.map(lambda y: add_count(y,c))\n",
    "    if(normalize):\n",
    "        normalize_counter(c)\n",
    "        \n",
    "    return c "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:1px solid black; padding:10px 10px; background-color: #A0E7E5; text-align: center; font-size:20px;\"> \n",
    "    <strong>Markov model with chords</strong>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can start by creating a Markov model where the basic elements are the chords. The problem is that with nearly 1000 different chords the number of possible transitions is huge (around $1000^{1000}$) and due to the relative small size of our dataset the results will be increadibly sparse (we will only show the first heatmap to illustrate that).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_verse_chords = count_bigrams(df = beats_collection_df, \n",
    "                            chord_representation = 'chord', \n",
    "                            subsection = 'verse',\n",
    "                            normalize = True)\n",
    "markov_heatmap(count_verse_chords, order = 'none', hide_diag = True)\n",
    "top_transition(count_verse_chords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chorus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_chorus_chords = count_bigrams(df = beats_collection_df, \n",
    "                            chord_representation = 'chord', \n",
    "                            subsection = 'chorus',\n",
    "                            normalize = True)\n",
    "#markov_heatmap(count_chorus_chords, order = 'none', hide_diag = True)\n",
    "top_transition(count_chorus_chords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Difference between chord transition of choruses and verses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dif = substract_counts(count_chorus_chords, count_verse_chords)\n",
    "#markov_heatmap(dif, order = 'none', hide_diag = True)\n",
    "top_transition(dif)\n",
    "top_transition(dif, position = 'tail')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:1px solid black; padding:10px 10px; background-color: #FFAEBC;\">\n",
    "    <strong>Conclusion:</strong> The Markov model for chords in itself doesn't tell us much about differences between verse and chorus, since differences are smaller than 1%.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:1px solid black; padding:10px 10px; background-color: #A0E7E5; text-align: center; font-size:20px;\"> \n",
    "    <strong>Markov model with roots of the chords</strong>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to lower the number of basic elements, we can look at just the roots of the chords used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_verse_roots = count_bigrams(df = beats_collection_df, \n",
    "                            chord_representation = 'root', \n",
    "                            subsection = 'verse',\n",
    "                            normalize = True)\n",
    "markov_heatmap(count_verse_roots, hide_diag = True)\n",
    "top_transition(count_verse_roots)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chorus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_chorus_roots = count_bigrams(df = beats_collection_df,\n",
    "                                   chord_representation = 'root', \n",
    "                                   subsection = 'chorus', \n",
    "                                   normalize = True)\n",
    "markov_heatmap(count_chorus_roots, hide_diag = True)\n",
    "top_transition(count_chorus_roots)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Difference between root chord transition of choruses and verses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dif = substract_counts(count_chorus_roots, count_verse_roots)\n",
    "markov_heatmap(dif, hide_diag = True)\n",
    "top_transition(dif)\n",
    "top_transition(dif, position = 'tail')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bonus : Difference between chord transition of choruses and intros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_intro_roots = count_bigrams(df = beats_collection_df,\n",
    "                                  chord_representation = 'root', \n",
    "                                  subsection = 'intro', \n",
    "                                  normalize = True)\n",
    "dif=substract_counts(count_chorus_roots, count_intro_roots)\n",
    "markov_heatmap(dif,hide_diag=True)\n",
    "top_transition(dif)\n",
    "top_transition(dif,position=\"tail\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:1px solid black; padding:10px 10px; background-color: #FFAEBC;\">\n",
    "    <strong>Conclusion:</strong> Looking just at the roots is certainly better but the differences are still fairly small. Notice how the diagonal shape is present when using the line of fifths order.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:1px solid black; padding:10px 10px; background-color: #FBE7C6;\"> \n",
    "    <div style = \"font-size:25px; text-align:center;\"><strong>Tonic distance and Musical path</strong></div>\n",
    "    <div>\n",
    "        The last axe of exploration revolves around the idea of musical path and tonic distance: how the song evolves through the verse and the chorus. In this section we will look at:\n",
    "            <ul>\n",
    "                <li>the chorus/verse proportion in songs</li>\n",
    "                <li>the tonic proportion</li>\n",
    "                <li>the per-chord tonic distance</li>\n",
    "                <li>the musical path in the songs (testing with different distance measures)</li>\n",
    "            </ul>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:1px solid black; padding:10px 10px; background-color: #A0E7E5; text-align: center; font-size:20px;\"> \n",
    "    <strong>Chorus/verse proportion</strong>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider the distribution of the proportion of beats in a song that belong to choruses, respectively to verses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOTAL_WEIGHT = \"total_weight\"\n",
    "\n",
    "d_collection_df[TOTAL_WEIGHT] = d_collection_df.apply(\\\n",
    "    lambda row: weight_row(row[METRE], row[DURATION], row[REPETITION]),axis=1)\n",
    "\n",
    "d_collection_df[N_SUCC_BEATS] =  d_collection_df.apply(\\\n",
    "    lambda row: successive_repetition_row(row[METRE], row[DURATION]),axis=1)\n",
    "\n",
    "d_collection_df = d_collection_df.astype({TOTAL_WEIGHT: 'Int64',N_SUCC_BEATS: 'Int64'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_beat_chorus = d_collection_df[d_collection_df.section_type == \"chorus\"].groupby(\"song_id\")[TOTAL_WEIGHT].sum()\\\n",
    ".reset_index().rename(columns= {TOTAL_WEIGHT:\"chorus_weight\"})\n",
    "n_beat_verse = d_collection_df[d_collection_df.section_type == \"verse\"].groupby(\"song_id\")[TOTAL_WEIGHT].sum()\\\n",
    ".reset_index().rename(columns= {TOTAL_WEIGHT:\"verse_weight\"})\n",
    "n_beat = d_collection_df.groupby(\"song_id\")[TOTAL_WEIGHT].sum()\n",
    "\n",
    "proportions_df = n_beat_chorus.merge(n_beat_verse,on=\"song_id\").merge(n_beat,on=\"song_id\")\n",
    "\n",
    "proportions_df[\"chorus_weight\"] = proportions_df[\"chorus_weight\"]/proportions_df[\"total_weight\"]\n",
    "proportions_df[\"verse_weight\"] = proportions_df[\"verse_weight\"]/proportions_df[\"total_weight\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proportions_df[[\"chorus_weight\",\"verse_weight\"]].describe().drop(\"count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:1px solid black; padding:10px 10px; background-color: #FFAEBC;\">\n",
    "    <strong>Conclusion:</strong> we can observe that the distributions of chorus and verse within songs share similar statistics, both accounting for a third of the songs.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:1px solid black; padding:10px 10px; background-color: #A0E7E5; text-align: center; font-size:20px;\"> \n",
    "    <strong>Tonic proportion</strong>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_collection_df[[SONG_ID,\"tonic\"]].drop_duplicates().groupby(\"tonic\")\\\n",
    ".count().rename(columns = {SONG_ID:\"number_of_songs\"}).plot.bar(figsize = (16,6), legend = False)\n",
    "plt.title('Tonic proportion of the songs in the corpus', fontsize = 20)\n",
    "plt.xticks(rotation = 'horizontal', fontsize =  14)\n",
    "plt.xlabel('Tonic', fontsize = 18)\n",
    "plt.ylabel('Number of songs', fontsize = 18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:1px solid black; padding:10px 10px; background-color: #A0E7E5; text-align: center; font-size:20px;\"> \n",
    "    <strong>Per-chord tonic distance analysis</strong>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From now, we consider d_collection_df, that contains only songs that share a chorus and a verse\n",
    "\n",
    "def per_chord_tonic_distance_analysis(distance,df,n_beat_type,btype):\n",
    "    \"\"\"\n",
    "    Create weighted mean and std of chord distance to tonic and describe them\n",
    "    \n",
    "    distance: measure of distance, for exemple relative tpc from the tonic\n",
    "    df: dataframe to consider\n",
    "    n_beat_type: dataframe that give for each song the number of beat (= number of chords) for given type\n",
    "    btype: name of the column in n_beat_type\n",
    "    \"\"\"\n",
    "    \n",
    "    df[\"weight*dist\"] =df[distance]*df[TOTAL_WEIGHT]\n",
    "    mean_numerator_df = df.groupby(SONG_ID)[\"weight*dist\"].sum().reset_index()\\\n",
    "    .rename(columns = {\"weight*dist\":\"mean_numerator\"})\n",
    "    \n",
    "    \n",
    "    agg_df = mean_numerator_df.merge(n_beat_type, on = \"song_id\")\n",
    "    agg_df[\"mean_distance\"] = agg_df[\"mean_numerator\"]/agg_df[btype]\n",
    "    \n",
    "    #Need mean to compute variance\n",
    "    df = df.merge(agg_df[[\"mean_distance\",\"song_id\"]], on = \"song_id\")\n",
    "    \n",
    "    df[\"variance_item\"] = (df[\"weight*dist\"] - df[TOTAL_WEIGHT]*df[\"mean_distance\"])\\\n",
    "    .apply(lambda x : np.power(x,2))\n",
    "    variance_numerator_df = df.groupby(SONG_ID)[\"variance_item\"].sum().reset_index()\\\n",
    "    .rename(columns = {\"variance_item\":\"variance_numerator\"})\n",
    "    \n",
    "    agg_df = agg_df.merge(variance_numerator_df, on = \"song_id\")\n",
    "    agg_df[\"variance_distance\"] = agg_df[\"variance_numerator\"]/agg_df[btype]\n",
    "    agg_df[\"std_distance\"] = agg_df[\"variance_distance\"].apply(np.sqrt)\n",
    "    \n",
    "    return agg_df.describe()[[\"mean_distance\",\"std_distance\"]]\n",
    "    \n",
    "    return mean_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_per_chord_tonic_distance_analysis(distance,df):\n",
    "    chorus_distance_df = df[df.section_type == \"chorus\"]\n",
    "    verse_distance_df = df[df.section_type == \"verse\"]\n",
    "    \n",
    "    n_beat_chorus = df[df.section_type == \"chorus\"].groupby(\"song_id\")[TOTAL_WEIGHT].sum()\\\n",
    "    .reset_index().rename(columns= {TOTAL_WEIGHT:\"chorus_weight\"})\n",
    "    n_beat_verse = df[df.section_type == \"verse\"].groupby(\"song_id\")[TOTAL_WEIGHT].sum()\\\n",
    "    .reset_index().rename(columns= {TOTAL_WEIGHT:\"verse_weight\"})\n",
    "    \n",
    "    print(\"Choruses\")\n",
    "    print(per_chord_tonic_distance_analysis(distance,chorus_distance_df,n_beat_chorus,\"chorus_weight\"))\n",
    "    print()\n",
    "    print(\"Verses\")\n",
    "    print(per_chord_tonic_distance_analysis(distance,verse_distance_df,n_beat_verse,\"verse_weight\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_per_chord_tonic_distance_analysis(\"relative_root_tpc\", d_collection_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:1px solid black; padding:10px 10px; background-color: #FFAEBC;\">\n",
    "    <strong>Conclusion:</strong> Results shows that the mean tpc distance to the tonic is higher in chorus sections than in verse sections. Statistical significance of this result should be confirmed, but it is an interesting path. We might perform further exploration by choosing a different measure of distance, for example related to how we perceive chords or to the tonal hierarchy. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:1px solid black; padding:10px 10px; background-color: #A0E7E5; text-align: center; font-size:20px;\"> \n",
    "    <strong>Musical path analysis</strong>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we explore the melodic lines of music by investigating how the distance to tonic evolve along the time dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Squeeze with repetition\n",
    "def weighted_squeeze(df,col):\n",
    "    \"\"\"\n",
    "    Recreate a line with one chord per beat\n",
    "    \"\"\"\n",
    "    df[\"chord_sublist\"] = df.apply(lambda row: row[N_SUCC_BEATS]*[row[col]],axis = 1)\n",
    "    \n",
    "    line_df = df.groupby([SONG_ID,TYPE,LINE_NUMBER,])[\"chord_sublist\"].sum().reset_index()\n",
    "    \n",
    "    \n",
    "    #line_df[\"chord_sublist\"] = line_df[\"chord_sublist\"].apply(lambda l: np.array(l).flatten())\n",
    "    \n",
    "    \n",
    "    return line_df.rename(columns = {\"chord_sublist\":\"chord_list\"})\n",
    "    \n",
    "def full_squeeze(df,col):\n",
    "    \"\"\"\n",
    "    Recreate a section, considering repetitions of full lines.\n",
    "    \"\"\"\n",
    "    #Not implemented yet\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_df = weighted_squeeze(d_collection_df,\"relative_root_tpc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_path(line_df,line_length,verbose=True):\n",
    "    \"\"\"\n",
    "    Show mean path for given line length\n",
    "    \"\"\"\n",
    "    \n",
    "    f = plt.figure(figsize=(12,4))\n",
    "    \n",
    "    means_dic = {}\n",
    "    \n",
    "    for i, section_type in enumerate((\"chorus\",\"verse\")):\n",
    "        \n",
    "        f.add_subplot(1,2,i+1)\n",
    "        \n",
    "        sample_df = line_df[line_df[\"chord_list\"].apply(lambda l: len(l) == line_length)]\n",
    "\n",
    "        sample_df = sample_df[sample_df[TYPE] == section_type]\n",
    "        \n",
    "        means = pd.DataFrame(sample_df[\"chord_list\"].values.tolist()).mean(axis = 0)\n",
    "        means_dic[section_type] = means\n",
    "        \n",
    "        if verbose:\n",
    "            print(len(sample_df),\"lines considered for\",section_type)\n",
    "\n",
    "            means.plot.bar(figsize = (16,4))\n",
    "            plt.title(\"Average relative root tpc for lines of length {} in {}\".format(line_length,section_type), fontsize = 16)\n",
    "            plt.xlabel(\"Position of chord\", fontsize = 14)\n",
    "            plt.xticks(rotation = 'horizontal', fontsize = 12)\n",
    "            plt.ylabel(\"Avg relative root tpc\", fontsize = 14)\n",
    "    \n",
    "    \n",
    "    if not verbose:\n",
    "        return means_dic    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_path(line_df,8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:1px solid black; padding:10px 10px; background-color: #FFAEBC;\">\n",
    "    <strong>Conclusion:</strong> Those graphs concentrate only on melodic lines of length 8, because they were numerous. However a normalization that would allow us to observe aggregations of melodic lines of all length still need to be found. This should therefore be considered more as an illustration than a fully representative graph.\n",
    "\n",
    "We observe the same result as the previous one: the path have a similar shape, but the pitch class is higher in the choruses. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_line_path(df,distance,line_len):\n",
    "    line_df = weighted_squeeze(df,distance)\n",
    "    mean_path(line_df,line_len)\n",
    "    \n",
    "def plot_section_path(df,distance,sec_len):\n",
    "    sec_df = full_squeeze(df,distance)\n",
    "    mean_path(sec_df,sec_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests with different distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distance between tonic and root of chord, in number of semitones up or down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_collection_df[\"tpc_distance\"] = d_collection_df[\"relative_root_tpc\"].apply(lambda tpc: min(tpc,12-tpc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_per_chord_tonic_distance_analysis(\"tpc_distance\",d_collection_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_line_path(d_collection_df,\"tpc_distance\",8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:1px solid black; padding:10px 10px; background-color: #FFAEBC;\">\n",
    "    <strong>Conclusion:</strong> Here, we compute a real distance in semitones between the tonic and the root of the chords, in the context of octave equivalence. This leads to the same conclusion: the distance to tonic looks larger in the choruses.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concordance distance (manual mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This small proximity attribution is done following the proximity graph of course 9 (slide 27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concordance_dic = {0:15,7:5,5:2.5,9:2.5,1:2.5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_collection_df[\"concordance_proximity\"] = d_collection_df[\"relative_root_tpc\"]\\\n",
    ".apply(lambda r: concordance_dic.get(r))\n",
    "\n",
    "d_collection_df = d_collection_df.fillna({\"concordance_proximity\":1}).dropna(subset = [\"relative_root_tpc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_per_chord_tonic_distance_analysis(\"concordance_proximity\",d_collection_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_line_path(d_collection_df,\"concordance_proximity\",8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:1px solid black; padding:10px 10px; background-color: #FFAEBC;\">\n",
    "    <strong>Conclusion:</strong> Again, the proximity is higher in the verses, suggesting a smaller distance to tonic. The shapes of both graphs show again interesting resemblance.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:1px solid black; padding:10px 10px; background-color: #FBE7C6;\"> \n",
    "    <div style = \"font-size:25px; text-align:center;\"><strong>Summary of Milestone 3</strong></div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this milestone, we have started by finishing all processing of the dataset in order to obtain a simple dataframe for all further analyses. Each row of the dataframe represents a chord played on a beat, with informations about all components of said chord (its root, its quality and its bass note, if one has been used). These informations have been extracted from the chord notation used in the dataset, defined by http://ismir2005.ismir.net/proceedings/1080.pdf (with some additions specific to this dataset). We have also only kept songs with both a verse and a chorus, resulting in a corpus of 712 songs. \n",
    "\n",
    "Once these final processing steps done, we have started our exploratory analysis. We have three main axis: \n",
    "* chord distributions\n",
    "* Markov models\n",
    "* tonic distance and musical path\n",
    "\n",
    "<strong>Chord distributions:</strong> For the first axe, most analyses have not been really interesting since differences between verse and chorus are fairly small. We expected to have more differences here but it could be that distributions are too simple of a way to compare the two sections. When moving to more advanced analysis however, using the bag of words representation to compute the mean distance between all chorus sections, all verse sections and whole songs, we found some interesting results. Indeed, it seems that the mean distance computed is smaller for verse and for chorus compared to the one of whole songs. This can be interpretated as chorus and verse having some characteristics across all songs that makes them similar and therefore recognizable. This of course is only a start, statistical tests would be required to confirm that and other distance measures should be used as well for comparison. In particular, the Jaccard similarity could be used to investigate whether chorus chords are a subset of verse chords.\n",
    "\n",
    "<strong>Markov models:</strong> When moving to the Markov models, results are quite difficult to interpret since differences are very small (less than 1%) and transitions from one chord to another are rather abstract. It is not that clear what further steps could be taken in this direction to get better results.\n",
    "\n",
    "<strong>Tonic distance and Musical path:</strong> Finally, the last axe has perhaps proven to be the most interesting. The idea is to define a tonic distance (such as the number of semitones between the chord root and the tonic of the song) and measure it over the song. Results show that the mean tonic distance seems higher in the chorus than in the verse. This relates to this idea of musical home, where composers adventure themselves further away from it in the chorus. This has led us to the concept of musical path, where we have investigated how the distance to tonic evolves along the time dimension. The analyses have been conducted on musical lines of length 8 and, surprisingly, when plotting the average  relative root tpc as a function of the position of the chord in the musical line, very similar shapes are found for verse and chorus sections. This is confirmed with different distance measures. This is certainly interesting enough to be pursued, especially by normalizing the results and testing the statistical significance of the differences observed.\n",
    "\n",
    "One thing we have not explored is the evolution over the years of songs. Since most of the results we have found are difficult to interpret or rely on very small differences, applying a time dimension over it and therefore reducing even further the size of the data sample for each analysis doens't seem to be the right way to go. However we plan to at least test this direction for our more interesting results, as it could still reveal itself interesting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:1px solid black; padding:10px 10px; background-color: #A0E7E5; text-align: center; font-size:20px;\"> \n",
    "    <strong>Final analysis for musical path analysis</strong>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beats_collection_df = beats_collection_df.astype({\"relative_root_tpc\":\"Int64\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temporal_squeeze(beats_collection_df, column, type, measurement = \"relative_root_tpc\"):\n",
    "    \n",
    "    selected_df = beats_collection_df[beats_collection_df.section_type == type]\n",
    "    \n",
    "    agg_df = selected_df.groupby([\"song_id\",column])[measurement].apply(list).reset_index()\n",
    "    \n",
    "    return agg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chorus_line_df = temporal_squeeze(beats_collection_df,\"line_id\",\"chorus\")\n",
    "chorus_section_df = temporal_squeeze(beats_collection_df,\"sequence_id\",\"chorus\")\n",
    "verse_line_df = temporal_squeeze(beats_collection_df,\"line_id\",\"verse\")\n",
    "verse_section_df = temporal_squeeze(beats_collection_df,\"sequence_id\",\"verse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heatmap of musical path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most common line and section length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histograms were really sparse, so we display just the most common lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_most_common(df, column, type, measurement=\"relative_root_tpc\"):\n",
    "    \n",
    "    df[\"length\"] = df[measurement].apply(len)\n",
    "    \n",
    "    print(\"*Number of {} per length for {}*\".format(column,type))\n",
    "    print(df.groupby(\"length\")[column].count().sort_values(ascending = False).head(10)\\\n",
    "     .reset_index().rename(columns = {column:\"Quantity\"}))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_most_common(chorus_line_df,\"line_id\",\"chorus\")\n",
    "show_most_common(verse_line_df,\"line_id\",\"verse\")\n",
    "show_most_common(chorus_section_df,\"sequence_id\",\"chorus\")\n",
    "show_most_common(verse_section_df,\"sequence_id\",\"verse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation: There are only a few lengths that occur enough to give meaningful results. We will in a first time consider the lengths for which there are more than 100 rows in both chorus and verse type. This means the 5 first lengths for the lines and the 3 first for choruses.\n",
    "\n",
    "Note also that we perform global comparison. We are not interested in comparing within one song the difference between the verses and the choruses, as most of the songs probably have verses and choruses of different lengths."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_lengths = (16,8,12,32,20)\n",
    "section_lengths = (32,64,48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_heatmap_df(df,length,measurement = \"relative_root_tpc\"):\n",
    "    \n",
    "    selected_df = df[df.length == length]\n",
    "    \n",
    "    heatmap_df = pd.DataFrame()\n",
    "    \n",
    "    for i in range(length):\n",
    "        \n",
    "        pname = \"pos {}\".format(i)\n",
    "        \n",
    "        selected_df[pname] = selected_df[measurement].apply(lambda l: l[i])\n",
    "        selected_df = selected_df.astype({pname:\"Int64\"})\n",
    "        \n",
    "        heatmap_df[pname] = selected_df[pname].value_counts()\n",
    "        \n",
    "        #Normalisation\n",
    "        heatmap_df = heatmap_df/heatmap_df.sum(axis = 0)\n",
    "        \n",
    "        heatmap_df.index.name = measurement\n",
    "        \n",
    "    return heatmap_df.sort_index(ascending=False)\n",
    "    \n",
    "        \n",
    "def create_heatmaps(chorus_df,verse_df,length,measurement = \"relative_root_tpc\"):\n",
    "    \"\"\"\n",
    "    Plot heatmaps for choruses, verses and difference\n",
    "    \"\"\"\n",
    "    heatmap_chorus = create_heatmap_df(chorus_df,length)\n",
    "    heatmap_verse = create_heatmap_df(verse_df,length)\n",
    "    heatmap_diff = (heatmap_chorus - heatmap_verse).sort_index(ascending=False)\n",
    "    \n",
    "    sns.heatmap(heatmap_chorus)\n",
    "    plt.title(\"Pitch class per position for choruses of length {}\".format(length))\n",
    "    plt.show()\n",
    "    \n",
    "    sns.heatmap(heatmap_verse)\n",
    "    plt.title(\"Pitch class per position for verses of length {}\".format(length))\n",
    "    plt.show()\n",
    "    \n",
    "    sns.heatmap(heatmap_diff)\n",
    "    plt.title(\"Difference in pitch class occurences (length {})\".format(length))\n",
    "    plt.show()\n",
    "    \n",
    "    return 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in line_lengths:\n",
    "    create_heatmaps(chorus_line_df,verse_line_df,i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in section_lengths:\n",
    "    create_heatmaps(chorus_section_df,verse_section_df,i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overlaying graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tonic distance in individual songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = create_heatmap_df(chorus_line_df,16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_heatmap_df(chorus_line_df,32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
